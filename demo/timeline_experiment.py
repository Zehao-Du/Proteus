#!/usr/bin/env python3
"""
ğŸ• Timeline Experiment - éªŒè¯ Network-Aware è°ƒåº¦çš„çœŸæ­£ä¼˜åŠ¿

æ ¸å¿ƒæ´å¯Ÿï¼š
- GPU ç”Ÿæˆé€Ÿåº¦å›ºå®šï¼Œä½† chunk åˆ°è¾¾å®¢æˆ·ç«¯çš„æ—¶é—´ä¸åŒ
- ç½‘ç»œå¥½ç”¨æˆ·ï¼šchunk ç«‹å³åˆ°è¾¾
- ç½‘ç»œå·®ç”¨æˆ·ï¼šchunk å»¶è¿Ÿåˆ°è¾¾
- Network-aware ä¼˜å…ˆç½‘ç»œå¥½ç”¨æˆ· â†’ ç´¯è®¡æœ‰æ•ˆ chunk æ›²çº¿ä¸€ç›´åœ¨ä¸Šé¢

è¾“å‡ºï¼šç´¯è®¡æœ‰æ•ˆ chunk éšæ—¶é—´å˜åŒ–çš„æ›²çº¿å›¾
"""

import argparse
import asyncio
import aiohttp
import json
import time
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass, field
from typing import List, Dict, Tuple
from collections import defaultdict
import sys


@dataclass
class UserProfile:
    """ç”¨æˆ·é…ç½®"""
    user_id: int
    rtt: float  # msï¼Œå½±å“ chunk åˆ°è¾¾æ—¶é—´
    health: float  # 0.0-1.0
    category: str  # 'very_bad', 'bad', 'good', 'very_good'


@dataclass
class ChunkEvent:
    """Chunk äº‹ä»¶ï¼šè®°å½•æ¯ä¸ª SSE chunk çš„è§‚æµ‹æ—¶é—´å’Œåˆæˆåˆ°è¾¾æ—¶é—´
    
    æ³¨æ„ï¼šobserved_arrival_time æ˜¯å®¢æˆ·ç«¯å®é™…æ”¶åˆ° SSE chunk çš„æ—¶é—´
    ï¼ˆåœ¨ localhost åœºæ™¯ä¸‹ï¼Œè¿‘ä¼¼ç­‰äº GPU ç”Ÿæˆæ—¶é—´ï¼‰
    synthetic_arrival_time æ˜¯åŠ å…¥ç½‘ç»œ RTT å»¶è¿Ÿåçš„"æœ‰æ•ˆåˆ°è¾¾æ—¶é—´"
    """
    user_id: int
    chunk_idx: int
    observed_arrival_time: float  # å®¢æˆ·ç«¯è§‚æµ‹åˆ°çš„åˆ°è¾¾æ—¶é—´ï¼ˆâ‰ˆ GPU ç”Ÿæˆæ—¶é—´ï¼Œlocalhostï¼‰
    synthetic_arrival_time: float  # åŠ å…¥ RTT å»¶è¿Ÿåçš„åˆæˆåˆ°è¾¾æ—¶é—´ï¼ˆç”¨äºè®¡ç®—æœ‰æ•ˆååï¼‰
    rtt: float
    category: str
    chunk_length: int  # chunk å†…å®¹é•¿åº¦


@dataclass
class RequestStats:
    """è¯·æ±‚ç»Ÿè®¡ï¼šè®°å½•æ¯ä¸ªè¯·æ±‚çš„å…³é”®æŒ‡æ ‡"""
    user_id: int
    category: str
    rtt: float
    profile_health: float  # ç”¨æˆ·é…ç½®çš„å¥åº·åº¦ï¼ˆprofile.healthï¼‰
    used_health_factor: float  # å®é™…ä½¿ç”¨çš„å¥åº·åº¦ï¼ˆbaseline=1.0, network-aware=profile.healthï¼‰
    ttft: float  # Time To First Chunkï¼ˆç§’ï¼‰
    total_chunks: int
    total_time: float  # è¯·æ±‚æ€»æ—¶é—´ï¼ˆç§’ï¼‰


def generate_user_profiles(num_users: int = 8192) -> List[UserProfile]:
    """ç”Ÿæˆç”¨æˆ·é…ç½® - ä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼ˆæœ‰é•¿å°¾ï¼‰
    
    RTT åˆ†å¸ƒï¼š
    - å‡å€¼ 400msï¼Œæ ‡å‡†å·® 1000ms
    - æˆªæ–­åˆ° [0, 800000] ms èŒƒå›´ï¼ˆæ¨¡æ‹Ÿæç«¯æƒ…å†µï¼šæœ‰äºº RTT å¾ˆå°ï¼Œæœ‰äººå¾ˆå¤§ï¼‰
    - å½¢æˆé•¿å°¾åˆ†å¸ƒï¼šå¤§å¤šæ•°ç”¨æˆ·ç½‘ç»œæ­£å¸¸ï¼Œå°‘æ•°ç”¨æˆ·ç½‘ç»œå¾ˆå·®
    
    å…³é”®ï¼šä½¿ç”¨ user_id ä½œä¸ºç§å­ï¼Œç¡®ä¿ä¸ Hint Server ä¸€è‡´ï¼
    """
    profiles = []
    
    for user_id in range(1, num_users + 1):
        # ä½¿ç”¨ user_id ä½œä¸ºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§
        # è¿™æ ·æ— è®ºåœ¨å“ªé‡Œè°ƒç”¨ï¼Œuser_id=N çš„ RTT éƒ½ç›¸åŒ
        np.random.seed(user_id + 42)  # +42 ä½œä¸ºåç§»
        
        # æ­£æ€åˆ†å¸ƒ RTTï¼šmean=400ms, std=1000ms
        rtt = np.random.normal(loc=400, scale=1000)
        rtt = float(np.clip(rtt, 0, 800000))
        
        # æ ¹æ® RTT è®¡ç®—å¥åº·åº¦ï¼šhealth = exp(-RTT / 500)
        # ä½¿ç”¨ 500 è€Œä¸æ˜¯ 150ï¼Œä»¥é€‚é…æ–°çš„ RTT åˆ†å¸ƒ (loc=400, scale=1000)
        health = float(np.exp(-rtt / 500.0))
        
        # æ ¹æ® RTT åˆ†ç±»
        if rtt >= 400:
            category = 'very_bad'
        elif rtt >= 200:
            category = 'bad'
        elif rtt >= 80:
            category = 'good'
        else:
            category = 'very_good'
        
        profiles.append(UserProfile(
            user_id=user_id,
            rtt=rtt,
            health=health,
            category=category
        ))
    
    # æ‰“ä¹±å‘é€é¡ºåºï¼Œä½† user_id â†’ RTT çš„æ˜ å°„ä¿æŒä¸å˜
    np.random.seed(999)
    np.random.shuffle(profiles)
    return profiles


class TimelineExperiment:
    """æ—¶é—´çº¿å®éªŒ"""
    
    def __init__(
        self,
        vllm_url: str = "http://localhost:8000/v1",
        num_users: int = 1024,
        max_tokens: int = 50,
        concurrency: int = 256,
        client_concurrency: int = 2048,
    ):
        self.vllm_url = vllm_url
        self.num_users = num_users
        self.max_tokens = max_tokens
        self.concurrency = concurrency  # vLLM çš„ max_num_seqs
        self.client_concurrency = client_concurrency  # å®¢æˆ·ç«¯å¹¶å‘è¿æ¥æ•°
        self.model_name = None
        
        # ç”Ÿæˆç”¨æˆ·é…ç½®ï¼ˆä½¿ç”¨å›ºå®šç§å­ä¿è¯ä¸¤æ¬¡å®éªŒç”¨æˆ·ç›¸åŒï¼‰
        np.random.seed(42)
        self.user_profiles = generate_user_profiles(num_users)
        
    async def detect_model(self, session: aiohttp.ClientSession) -> str:
        """æ£€æµ‹æ¨¡å‹åç§°"""
        try:
            async with session.get(f"{self.vllm_url}/models") as resp:
                if resp.status == 200:
                    data = await resp.json()
                    models = data.get("data", [])
                    if models:
                        return models[0].get("id", "unknown")
        except:
            pass
        return "unknown"
    
    async def send_request(
        self,
        session: aiohttp.ClientSession,
        profile: UserProfile,
        experiment_start_time: float,
        semaphore: asyncio.Semaphore,
        mode: str,
    ) -> Tuple[List[ChunkEvent], RequestStats]:
        """å‘é€å•ä¸ªè¯·æ±‚ï¼Œè®°å½•æ¯ä¸ª chunk çš„äº‹ä»¶å’Œ TTFC
        
        ä½¿ç”¨ Semaphore é™åˆ¶å®¢æˆ·ç«¯å¹¶å‘ï¼Œé¿å…è¿æ¥é£æš´
        ä½†å¹¶å‘æ•°è¶³å¤Ÿå¤§ï¼Œè®© backlog è¿›å…¥ vLLM çš„ waiting é˜Ÿåˆ—
        """
        events = []
        request_start_time = time.perf_counter()
        first_chunk_time = None
        
        prompt = f"User {profile.user_id}: Write a brief story about AI."
        
        # ä½¿ç”¨åŒ…å« user_id çš„æ ¼å¼ï¼Œè®© vLLM çš„ _extract_user_id èƒ½è¯†åˆ«
        # æ ¼å¼: user{N}_{random} -> vLLM å¯ä»¥æå– user_id = N
        import uuid
        custom_request_id = f"user{profile.user_id}_{uuid.uuid4().hex[:8]}"
        
        # å…³é”®ï¼šBaseline æ¨¡å¼ä¼  health_factor=1.0ï¼ŒNetwork-Aware æ¨¡å¼ä¼  profile.health
        health_factor = 1.0 if mode == "baseline" else profile.health
        
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": self.max_tokens,
            "stream": True,
            "temperature": 0.0,  # å›ºå®šé‡‡æ ·ï¼Œç¡®ä¿å¯å¤ç°
            "top_p": 1.0,  # å›ºå®šé‡‡æ ·
            "ignore_eos": True,  # å¼ºåˆ¶ç”Ÿæˆ max_tokensï¼Œé¿å…æå‰ç»“æŸ
            "user": f"user{profile.user_id}",
            "request_id": custom_request_id,  # vLLM æ”¯æŒè‡ªå®šä¹‰ request_idï¼
            # ç›´æ¥ä¼ é€’å¥åº·åº¦ï¼Œé¿å…æŸ¥è¯¢ hint server
            "vllm_xargs": {
                "health_factor": health_factor
            }
        }
        
        # ä½¿ç”¨ Semaphore æ§åˆ¶å¹¶å‘
        async with semaphore:
            try:
                async with session.post(
                    f"{self.vllm_url}/chat/completions",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=300),
                ) as resp:
                    if resp.status != 200:
                        return events, None
                    
                    chunk_idx = 0
                    
                    async for line in resp.content:
                        if not line:
                            continue
                        
                        line_str = line.decode('utf-8').strip()
                        if not line_str.startswith("data: "):
                            continue
                        
                        data_str = line_str[6:]
                        if data_str == "[DONE]":
                            break
                        
                        try:
                            data = json.loads(data_str)
                            choices = data.get("choices", [])
                            if choices:
                                delta = choices[0].get("delta", {})
                                content = delta.get("content")
                                if content:
                                    current_time = time.perf_counter()
                                    
                                    # è®°å½• TTFCï¼ˆç¬¬ä¸€ä¸ª chunk çš„æ—¶é—´ï¼‰
                                    if first_chunk_time is None:
                                        first_chunk_time = current_time - request_start_time
                                    
                                    # è§‚æµ‹åˆ°çš„åˆ°è¾¾æ—¶é—´ï¼ˆå®¢æˆ·ç«¯æ”¶åˆ° SSE chunk çš„æ—¶é—´ï¼‰
                                    # åœ¨ localhost åœºæ™¯ä¸‹ï¼Œè¿™è¿‘ä¼¼ç­‰äº GPU ç”Ÿæˆæ—¶é—´
                                    observed_arrival_time = current_time - experiment_start_time
                                    
                                    # åˆæˆåˆ°è¾¾æ—¶é—´ï¼šåŠ å…¥ç½‘ç»œ RTT å»¶è¿Ÿåçš„"æœ‰æ•ˆåˆ°è¾¾æ—¶é—´"
                                    # ç”¨äºè®¡ç®—æœ‰æ•ˆååï¼ˆECPS: Effective Chunks Per Secondï¼‰
                                    synthetic_arrival_time = observed_arrival_time + (profile.rtt / 1000.0 / 2.0)
                                    
                                    events.append(ChunkEvent(
                                        user_id=profile.user_id,
                                        chunk_idx=chunk_idx,
                                        observed_arrival_time=observed_arrival_time,
                                        synthetic_arrival_time=synthetic_arrival_time,
                                        rtt=profile.rtt,
                                        category=profile.category,
                                        chunk_length=len(content)
                                    ))
                                    chunk_idx += 1
                        except json.JSONDecodeError:
                            continue
            except Exception as e:
                pass
        
        # åˆ›å»ºè¯·æ±‚ç»Ÿè®¡
        total_time = time.perf_counter() - request_start_time
        # å®é™…ä½¿ç”¨çš„å¥åº·åº¦ï¼šbaseline=1.0, network-aware=profile.health
        used_health_factor = 1.0 if mode == "baseline" else profile.health
        
        stats = RequestStats(
            user_id=profile.user_id,
            category=profile.category,
            rtt=profile.rtt,
            profile_health=profile.health,  # ç”¨æˆ·é…ç½®çš„å¥åº·åº¦
            used_health_factor=used_health_factor,  # å®é™…ä½¿ç”¨çš„å¥åº·åº¦
            ttft=first_chunk_time if first_chunk_time else 0,
            total_chunks=len(events),
            total_time=total_time
        ) if events else None
        
        return events, stats
    
    async def run_experiment(
        self, 
        mode: str, 
        shuffled_profiles: List[UserProfile]
    ) -> Tuple[List[ChunkEvent], List[RequestStats], float]:
        """è¿è¡Œå®éªŒï¼Œè¿”å›æ‰€æœ‰ chunk äº‹ä»¶å’Œè¯·æ±‚ç»Ÿè®¡
        
        Args:
            mode: "baseline" æˆ– "network_aware"
            shuffled_profiles: å·²æ‰“ä¹±çš„ç”¨æˆ·é…ç½®åˆ—è¡¨ï¼ˆä¸¤ä¸ªå®éªŒå¤ç”¨åŒä¸€ä¸ªé¡ºåºï¼‰
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ Running {mode.upper()} - {self.num_users} Users")
        print(f"{'='*60}")
        
        # è®¾ç½® TCPConnector limitï¼Œé¿å… aiohttp è‡ªå·±æ— é™å¼€è¿æ¥
        connector = aiohttp.TCPConnector(limit=self.client_concurrency)
        async with aiohttp.ClientSession(connector=connector) as session:
            if not self.model_name:
                self.model_name = await self.detect_model(session)
                print(f"ğŸ“¦ Model: {self.model_name}")
            
            print(f"ğŸ”¢ Users: {self.num_users}")
            print(f"ğŸ¯ vLLM max_num_seqs (assumed): {self.concurrency}")
            print(f"ğŸ”Œ Client concurrency: {self.client_concurrency}")
            print(f"ğŸ“ Max tokens: {self.max_tokens}")
            print(f"âš ï¸  Note: max_num_seqs must match vLLM server config")
            
            experiment_start_time = time.perf_counter()
            
            # ä½¿ç”¨ Semaphore é™åˆ¶å®¢æˆ·ç«¯å¹¶å‘è¿æ¥æ•°
            # ç­–ç•¥ï¼šè¶³å¤Ÿå¤§ä½†æœ‰é™ï¼Œè®© backlog è¿›å…¥ vLLM çš„ waiting é˜Ÿåˆ—
            # ä½†ä¸ä¼šå‹å®ç³»ç»Ÿï¼ˆé¿å…è¿æ¥é£æš´å’Œ IO ç“¶é¢ˆï¼‰
            semaphore = asyncio.Semaphore(self.client_concurrency)
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡ï¼ˆä½†å— Semaphore æ§åˆ¶å¹¶å‘æ‰§è¡Œï¼‰
            tasks = [
                self.send_request(session, profile, experiment_start_time, semaphore, mode)
                for profile in shuffled_profiles
            ]
            
            # è®¡ç®—é¢„æœŸçš„ waiting é˜Ÿåˆ—è§„æ¨¡
            # ç”±äº Semaphore é™åˆ¶ï¼ŒåŒä¸€æ—¶åˆ»æœ€å¤š client_concurrency ä¸ªè¯·æ±‚è¿›å…¥ vLLM
            # å…¶ä¸­ concurrency ä¸ªåœ¨ runningï¼Œå…¶ä½™åœ¨ waiting
            expected_backlog = max(0, self.client_concurrency - self.concurrency)
            
            print(f"\nâ³ Sending {len(tasks)} requests...")
            print(f"   Client concurrency: {self.client_concurrency} (Semaphore)")
            print(f"   vLLM max_num_seqs: {self.concurrency} (scheduler limit)")
            print(f"   Expected backlog: ~{expected_backlog} requests in waiting queue")
            print(f"   Health factor: {'1.0 (all users)' if mode == 'baseline' else 'varies by RTT'}")
            results = await asyncio.gather(*tasks)
            
            duration = time.perf_counter() - experiment_start_time
        
        # åˆå¹¶æ‰€æœ‰äº‹ä»¶å’Œç»Ÿè®¡
        all_events = []
        all_stats = []
        completed_requests = 0
        failed_requests = 0
        for events, stats in results:
            if stats and len(events) > 0:
                all_events.extend(events)
                all_stats.append(stats)
                completed_requests += 1
            else:
                failed_requests += 1
        
        # è®¡ç®— TTFC ç»Ÿè®¡
        if all_stats:
            ttfts = [s.ttft for s in all_stats if s.ttft > 0]  # ttft å­—æ®µåä¿ç•™ï¼Œä½†å®é™…æ˜¯ TTFC
            if ttfts:
                avg_ttft = np.mean(ttfts)
                p50_ttft = np.percentile(ttfts, 50)
                p95_ttft = np.percentile(ttfts, 95)
                p99_ttft = np.percentile(ttfts, 99)
                
                # Chunk é•¿åº¦éªŒè¯ï¼ˆæ£€æŸ¥æ˜¯å¦ä¸€ token ä¸€ chunkï¼‰
                chunk_lengths = [e.chunk_length for e in all_events]
                if chunk_lengths:
                    avg_chunk_len = np.mean(chunk_lengths)
                    max_chunk_len = max(chunk_lengths)
                    chunk_len_dist = {1: sum(1 for l in chunk_lengths if l == 1),
                                    2: sum(1 for l in chunk_lengths if l == 2),
                                    3: sum(1 for l in chunk_lengths if l == 3),
                                    '>3': sum(1 for l in chunk_lengths if l > 3)}
                
                print(f"ğŸ“Š Total chunks: {len(all_events)}")
                print(f"âœ… Completed requests: {completed_requests}/{len(results)}")
                print(f"âŒ Failed requests: {failed_requests}/{len(results)}")
                print(f"â±ï¸  Duration: {duration:.2f}s")
                
                # Chunk é•¿åº¦ç»Ÿè®¡ï¼ˆç”¨äºéªŒè¯ chunk å¤§å°åˆ†å¸ƒï¼‰
                if chunk_lengths:
                    print(f"\nğŸ“ Chunk Length Statistics:")
                    print(f"   Avg: {avg_chunk_len:.2f} chars")
                    print(f"   Max: {max_chunk_len} chars")
                    print(f"   Distribution: 1 char={chunk_len_dist[1]}, 2 chars={chunk_len_dist[2]}, 3 chars={chunk_len_dist[3]}, >3 chars={chunk_len_dist['>3']}")
                
                print(f"\nâš¡ TTFC (Time To First Chunk) Statistics:")
                print(f"   Avg: {avg_ttft*1000:.1f}ms")
                print(f"   P50: {p50_ttft*1000:.1f}ms")
                print(f"   P95: {p95_ttft*1000:.1f}ms")
                print(f"   P99: {p99_ttft*1000:.1f}ms")
                
                # æŒ‰ç±»åˆ«ç»Ÿè®¡ TTFC
                print(f"\nâš¡ TTFC by Category:")
                for cat in ['very_good', 'good', 'bad', 'very_bad']:
                    cat_ttfts = [s.ttft for s in all_stats if s.category == cat and s.ttft > 0]
                    if cat_ttfts:
                        print(f"   {cat:10s}: Avg={np.mean(cat_ttfts)*1000:.1f}ms, P50={np.percentile(cat_ttfts, 50)*1000:.1f}ms, P95={np.percentile(cat_ttfts, 95)*1000:.1f}ms")
        else:
            print(f"ğŸ“Š Total chunks: {len(all_events)}")
            print(f"â±ï¸  Duration: {duration:.2f}s")
        
        return all_events, all_stats, duration
    
    def compute_cumulative_curve(
        self, 
        events: List[ChunkEvent], 
        time_points: np.ndarray,
        use_synthetic_arrival: bool = True
    ) -> np.ndarray:
        """è®¡ç®—ç´¯è®¡æœ‰æ•ˆ chunk æ›²çº¿
        
        Args:
            events: chunk äº‹ä»¶åˆ—è¡¨
            time_points: æ—¶é—´é‡‡æ ·ç‚¹
            use_synthetic_arrival: True ä½¿ç”¨åˆæˆåˆ°è¾¾æ—¶é—´ï¼ˆåŠ å…¥ RTTï¼‰ï¼ŒFalse ä½¿ç”¨è§‚æµ‹åˆ°è¾¾æ—¶é—´ï¼ˆâ‰ˆ GPU ç”Ÿæˆæ—¶é—´ï¼‰
        """
        cumulative = np.zeros(len(time_points))
        
        if use_synthetic_arrival:
            times = sorted([e.synthetic_arrival_time for e in events])
        else:
            times = sorted([e.observed_arrival_time for e in events])
        
        event_idx = 0
        for i, t in enumerate(time_points):
            while event_idx < len(times) and times[event_idx] <= t:
                event_idx += 1
            cumulative[i] = event_idx
        
        return cumulative
    
    async def run_comparison(self):
        """è¿è¡Œå¯¹æ¯”å®éªŒ"""
        print("\n" + "ğŸ•" * 30)
        print("     TIMELINE EXPERIMENT")
        print("     éªŒè¯ Network-Aware çš„çœŸæ­£ä¼˜åŠ¿")
        print("ğŸ•" * 30)
        
        # ç”Ÿæˆç”¨æˆ·é…ç½®ï¼ˆä¸¤æ¬¡å®éªŒç”¨ç›¸åŒçš„ç”¨æˆ·ï¼‰
        np.random.seed(42)
        self.user_profiles = generate_user_profiles(self.num_users)
        
        # æ‰“å°ç”¨æˆ·åˆ†å¸ƒ
        categories = {}
        for p in self.user_profiles:
            categories[p.category] = categories.get(p.category, 0) + 1
        print(f"\nğŸ“Š ç”¨æˆ·åˆ†å¸ƒ: {categories}")
        print(f"   RTT èŒƒå›´: {min(p.rtt for p in self.user_profiles):.1f} - {max(p.rtt for p in self.user_profiles):.1f} ms")
        print(f"   Health èŒƒå›´: {min(p.health for p in self.user_profiles):.3f} - {max(p.health for p in self.user_profiles):.3f}")
        
        # å…³é”®ï¼šç”Ÿæˆå›ºå®šçš„è¯·æ±‚åˆ°è¾¾é¡ºåºï¼Œä¸¤ä¸ªå®éªŒå¤ç”¨
        # è¿™æ · baseline å’Œ network-aware çš„åˆ°è¾¾é¡ºåºå®Œå…¨ä¸€è‡´ï¼Œå¯¹æ¯”æ‰æœ‰æ„ä¹‰
        import random
        shuffled_profiles = self.user_profiles.copy()
        random.Random(12345).shuffle(shuffled_profiles)  # å›ºå®šç§å­ï¼Œä¿è¯å¯å¤ç°
        print(f"\nğŸ”€ Request arrival order: Fixed seed (12345) for both experiments")
        
        # Baselineï¼šæ‰€æœ‰ç”¨æˆ· health=1.0ï¼ŒFCFS è°ƒåº¦
        baseline_events, baseline_stats, baseline_duration = await self.run_experiment("baseline", shuffled_profiles)
        
        await asyncio.sleep(3)
        
        # Network-Awareï¼šæ ¹æ® RTT è®¡ç®— healthï¼Œä¼˜å…ˆè°ƒåº¦å¥åº·åº¦é«˜çš„ç”¨æˆ·
        network_events, network_stats, network_duration = await self.run_experiment("network_aware", shuffled_profiles)
        
        # è®¡ç®—æ›²çº¿
        max_time = max(baseline_duration, network_duration)
        time_points = np.linspace(0, max_time, 500)
        
        # ç´¯è®¡è§‚æµ‹åˆ°çš„ chunkï¼ˆGPU è§†è§’ï¼Œåº”è¯¥ç›¸åŒï¼‰
        # ä½¿ç”¨ observed_arrival_timeï¼ˆåœ¨ localhost åœºæ™¯ä¸‹ â‰ˆ GPU ç”Ÿæˆæ—¶é—´ï¼‰
        baseline_observed = self.compute_cumulative_curve(baseline_events, time_points, use_synthetic_arrival=False)
        network_observed = self.compute_cumulative_curve(network_events, time_points, use_synthetic_arrival=False)
        
        # ç´¯è®¡æœ‰æ•ˆåˆ°è¾¾çš„ chunkï¼ˆå®¢æˆ·ç«¯è§†è§’ï¼ŒåŠ å…¥ RTT å»¶è¿Ÿï¼Œåº”è¯¥ä¸åŒï¼ï¼‰
        # ä½¿ç”¨ synthetic_arrival_timeï¼ˆåŠ å…¥ç½‘ç»œ RTT åçš„æœ‰æ•ˆåˆ°è¾¾æ—¶é—´ï¼‰
        baseline_arrived = self.compute_cumulative_curve(baseline_events, time_points, use_synthetic_arrival=True)
        network_arrived = self.compute_cumulative_curve(network_events, time_points, use_synthetic_arrival=True)
        
        # ç»˜å›¾
        plt.figure(figsize=(14, 10))
        
        # å­å›¾1ï¼šç´¯è®¡è§‚æµ‹åˆ°çš„ chunkï¼ˆGPU è§†è§’ï¼Œlocalhost â‰ˆ ç”Ÿæˆæ—¶é—´ï¼‰
        plt.subplot(2, 2, 1)
        plt.plot(time_points, baseline_observed, 'r-', label='Baseline', linewidth=2)
        plt.plot(time_points, network_observed, 'g-', label='Network-Aware', linewidth=2)
        plt.xlabel('Time (s)')
        plt.ylabel('Cumulative Chunks Observed')
        plt.title('GPU Perspective: Cumulative Chunks Observed\n(localhost â‰ˆ generation time, both should be identical)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­å›¾2ï¼šç´¯è®¡åˆ°è¾¾çš„ chunkï¼ˆå®¢æˆ·ç«¯è§†è§’ï¼‰
        plt.subplot(2, 2, 2)
        plt.plot(time_points, baseline_arrived, 'r-', label='Baseline', linewidth=2)
        plt.plot(time_points, network_arrived, 'g-', label='Network-Aware', linewidth=2)
        plt.xlabel('Time (s)')
        plt.ylabel('Cumulative Chunks Arrived')
        plt.title('Client Perspective: Cumulative Chunks Arrived\n(Network-Aware should always be on top!)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­å›¾3ï¼šå·®å€¼ï¼ˆNetwork-Aware - Baselineï¼‰
        plt.subplot(2, 2, 3)
        diff = network_arrived - baseline_arrived
        plt.fill_between(time_points, 0, diff, where=(diff > 0), color='green', alpha=0.5, label='Network-Aware é¢†å…ˆ')
        plt.fill_between(time_points, 0, diff, where=(diff < 0), color='red', alpha=0.5, label='Baseline é¢†å…ˆ')
        plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        plt.xlabel('Time (s)')
        plt.ylabel('Chunk Difference')
        plt.title('Difference: Network-Aware - Baseline\n(Positive = Network-Aware leads)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­å›¾4ï¼šECPS (Effective Chunks Per Second) éšæ—¶é—´å˜åŒ–
        plt.subplot(2, 2, 4)
        # é¿å…é™¤ä»¥0
        etps_baseline = np.zeros_like(time_points)
        etps_network = np.zeros_like(time_points)
        for i, t in enumerate(time_points):
            if t > 0.5:  # ä»0.5ç§’å¼€å§‹è®¡ç®—
                etps_baseline[i] = baseline_arrived[i] / t
                etps_network[i] = network_arrived[i] / t
        
        plt.plot(time_points, etps_baseline, 'r-', label='Baseline ECPS', linewidth=2)
        plt.plot(time_points, etps_network, 'g-', label='Network-Aware ECPS', linewidth=2)
        plt.xlabel('Time (s)')
        plt.ylabel('ECPS (Effective Chunks Per Second)')
        plt.title('ECPS Over Time\n(Network-Aware should always be higher!)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('/home/v-boxiuli/eBPF-TokenFlow/demo/timeline_comparison.png', dpi=150)
        print(f"\nğŸ“ˆ å›¾è¡¨å·²ä¿å­˜åˆ°: /home/v-boxiuli/eBPF-TokenFlow/demo/timeline_comparison.png")
        
        # æ‰“å°ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ“Š STATISTICAL SUMMARY")
        print("=" * 60)
        
        # æ‰¾åˆ°ä¸­é—´æ—¶é—´ç‚¹çš„æ•°æ®
        mid_idx = len(time_points) // 2
        mid_time = time_points[mid_idx]
        
        print(f"\nåœ¨ t={mid_time:.1f}s æ—¶:")
        print(f"  Baseline åˆ°è¾¾: {int(baseline_arrived[mid_idx])} chunks")
        print(f"  Network-Aware åˆ°è¾¾: {int(network_arrived[mid_idx])} chunks")
        print(f"  å·®å€¼: {int(diff[mid_idx])} chunks ({diff[mid_idx]/max(baseline_arrived[mid_idx],1)*100:+.1f}%)")
        
        # æœ€ç»ˆç»“æœ
        print(f"\næœ€ç»ˆç»“æœ (t={max_time:.1f}s):")
        print(f"  Baseline åˆ°è¾¾: {int(baseline_arrived[-1])} chunks")
        print(f"  Network-Aware åˆ°è¾¾: {int(network_arrived[-1])} chunks")
        print(f"  å·®å€¼: {int(diff[-1])} chunks")
        
        # å¹³å‡é¢†å…ˆé‡
        avg_lead = np.mean(diff)
        print(f"\nå¹³å‡é¢†å…ˆé‡: {avg_lead:.1f} chunks")
        print(f"é¢†å…ˆæ—¶é—´æ¯”ä¾‹: {np.mean(diff > 0)*100:.1f}%")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        print(f"\nğŸ“Š æŒ‰ç”¨æˆ·ç±»åˆ«ç»Ÿè®¡:")
        for cat in ['very_bad', 'bad', 'good', 'very_good']:
            b_count = len([e for e in baseline_events if e.category == cat])
            n_count = len([e for e in network_events if e.category == cat])
            # å¹³å‡å»¶è¿Ÿ = synthetic_arrival_time - observed_arrival_time = RTT/2
            b_avg_delay = np.mean([e.synthetic_arrival_time - e.observed_arrival_time for e in baseline_events if e.category == cat]) * 1000 if b_count > 0 else 0
            n_avg_delay = np.mean([e.synthetic_arrival_time - e.observed_arrival_time for e in network_events if e.category == cat]) * 1000 if n_count > 0 else 0
            print(f"  {cat:10s}: Baseline {b_count:5d} chunks (avg delay {b_avg_delay:.0f}ms), Network-Aware {n_count:5d} chunks (avg delay {n_avg_delay:.0f}ms)")
        
        # TTFT å¯¹æ¯”ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("âš¡ TTFT COMPARISON")
        print("=" * 60)
        
        if baseline_stats and network_stats:
            b_ttfts = [s.ttft for s in baseline_stats if s.ttft > 0]
            n_ttfts = [s.ttft for s in network_stats if s.ttft > 0]
            
            if b_ttfts and n_ttfts:
                print(f"\nğŸ“Š Overall TTFT:")
                print(f"  Baseline:      Avg={np.mean(b_ttfts)*1000:.1f}ms, P50={np.percentile(b_ttfts, 50)*1000:.1f}ms, P95={np.percentile(b_ttfts, 95)*1000:.1f}ms, P99={np.percentile(b_ttfts, 99)*1000:.1f}ms")
                print(f"  Network-Aware: Avg={np.mean(n_ttfts)*1000:.1f}ms, P50={np.percentile(n_ttfts, 50)*1000:.1f}ms, P95={np.percentile(n_ttfts, 95)*1000:.1f}ms, P99={np.percentile(n_ttfts, 99)*1000:.1f}ms")
                
                ttft_improvement = (np.mean(b_ttfts) - np.mean(n_ttfts)) / np.mean(b_ttfts) * 100
                print(f"\n  TTFT Improvement: {ttft_improvement:+.1f}%")
                
                # æŒ‰ç±»åˆ«ç»Ÿè®¡ TTFT
                print(f"\nğŸ“Š TTFT by Category:")
                for cat in ['very_good', 'good', 'bad', 'very_bad']:
                    b_cat_ttfts = [s.ttft for s in baseline_stats if s.category == cat and s.ttft > 0]
                    n_cat_ttfts = [s.ttft for s in network_stats if s.category == cat and s.ttft > 0]
                    
                    if b_cat_ttfts and n_cat_ttfts:
                        b_avg = np.mean(b_cat_ttfts) * 1000
                        n_avg = np.mean(n_cat_ttfts) * 1000
                        improvement = (b_avg - n_avg) / b_avg * 100 if b_avg > 0 else 0
                        print(f"  {cat:10s}: Baseline Avg={b_avg:.1f}ms, Network-Aware Avg={n_avg:.1f}ms ({improvement:+.1f}%)")


async def main():
    parser = argparse.ArgumentParser(description="Timeline Experiment")
    parser.add_argument("--vllm-url", default="http://localhost:8000/v1")
    parser.add_argument("--num-users", type=int, default=8192)
    parser.add_argument("--max-tokens", type=int, default=50)
    parser.add_argument("--concurrency", type=int, default=256, 
                       help="vLLM max_num_seqs (scheduler limit)")
    parser.add_argument("--client-concurrency", type=int, default=2048,
                       help="Client-side concurrency limit (Semaphore)")
    args = parser.parse_args()
    
    experiment = TimelineExperiment(
        vllm_url=args.vllm_url,
        num_users=args.num_users,
        max_tokens=args.max_tokens,
        concurrency=args.concurrency,
        client_concurrency=args.client_concurrency,
    )
    
    await experiment.run_comparison()


if __name__ == "__main__":
    asyncio.run(main())

