# çœŸå® LLM å¼•æ“é›†æˆä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ `real_llm_client.py` ä¸çœŸå®çš„ LLM æ¨ç†å¼•æ“ï¼ˆvLLM æˆ– Ollamaï¼‰è¿›è¡Œç½‘ç»œæ„ŸçŸ¥çš„ Token ç”Ÿæˆã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

1. **Hint Server æ­£åœ¨è¿è¡Œ**
   ```bash
   cd demo
   sudo bash run_demo.sh  # è¿™ä¼šå¯åŠ¨ Hint Serverï¼ˆåœ¨åå°ï¼‰
   # æˆ–è€…å•ç‹¬å¯åŠ¨ï¼š
   python hint_server.py --iso-model ../agent/isolation_forest.pkl \
                          --gbdt-model ../agent/gbdt_model.pkl \
                          --data-path ../data/net_data.csv
   ```

2. **eBPF æ•°æ®é‡‡é›†æ­£åœ¨è¿è¡Œ**ï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰
   ```bash
   cd data_collection
   sudo bash collect_data.sh
   ```

3. **é€‰æ‹©å¹¶å®‰è£… LLM å¼•æ“**

## ğŸ”§ é€‰é¡¹ 1ï¼šä½¿ç”¨ Ollamaï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰

### å®‰è£… Ollama

```bash
# Ubuntu/Debian
curl -fsSL https://ollama.com/install.sh | sh

# æˆ–ä»å®˜ç½‘ä¸‹è½½ï¼šhttps://ollama.com/download
```

### å¯åŠ¨ Ollama æœåŠ¡

```bash
ollama serve
```

### ä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ç”¨äºæµ‹è¯•
ollama pull llama2

# æˆ–ä¸‹è½½å…¶ä»–æ¨¡å‹
ollama pull mistral
ollama pull codellama
```

### è¿è¡Œå®¢æˆ·ç«¯

```bash
cd demo
python real_llm_client.py \
    --engine ollama \
    --ollama-model llama2 \
    --prompt "Tell me a short story about network optimization" \
    --max-tokens 200
```

## ğŸ”§ é€‰é¡¹ 2ï¼šä½¿ç”¨ vLLMï¼ˆé«˜æ€§èƒ½ï¼‰

### å®‰è£… vLLM

```bash
pip install vllm
# æˆ–ä»æºç å®‰è£…
# git clone https://github.com/vllm-project/vllm.git
# cd vllm && pip install -e .
```

### å¯åŠ¨ vLLM æœåŠ¡

```bash
# ä½¿ç”¨ OpenAI å…¼å®¹ API
python -m vllm.entrypoints.openai.api_server \
    --model <your-model-path> \
    --port 8000

# ä¾‹å¦‚ä½¿ç”¨ HuggingFace æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --port 8000
```

### è¿è¡Œå®¢æˆ·ç«¯

```bash
cd demo
python real_llm_client.py \
    --engine vllm \
    --vllm-url http://localhost:8000/v1 \
    --vllm-model default \
    --prompt "Tell me a short story about network optimization" \
    --max-tokens 200
```

## ğŸš€ ä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

ç¼–è¾‘ `run_real_llm.sh` é…ç½®ä½ çš„å¼•æ“å’Œå‚æ•°ï¼Œç„¶åè¿è¡Œï¼š

```bash
cd demo
bash run_real_llm.sh
```

## ğŸ“Š è§‚å¯Ÿæ•ˆæœ

å½“è¿è¡Œ `real_llm_client.py` æ—¶ï¼Œä½ ä¼šçœ‹åˆ°ï¼š

1. **åˆå§‹è¿æ¥ä¿¡æ¯**ï¼šæ˜¾ç¤º Hint Server è¿æ¥çŠ¶æ€å’Œåˆå§‹é€Ÿç‡
2. **æµå¼è¾“å‡º**ï¼šå®æ—¶æ˜¾ç¤º LLM ç”Ÿæˆçš„ Token
3. **é€Ÿç‡ä¿¡æ¯**ï¼šæ¯ 10 ä¸ª Token æ˜¾ç¤ºä¸€æ¬¡å½“å‰é€Ÿç‡å’Œç½‘ç»œå¥åº·åº¦
4. **ç»Ÿè®¡ä¿¡æ¯**ï¼šç”Ÿæˆå®Œæˆåæ˜¾ç¤ºæ€» Token æ•°ã€å®é™…é€Ÿç‡ã€ç›®æ ‡é€Ÿç‡å’Œç½‘ç»œæŒ‡æ ‡

### ç¤ºä¾‹è¾“å‡º

```
ğŸ”— Connecting to Hint Server: http://localhost:5000/hint
âœ… Initial rate: 45.2 tps, Health: 0.85
ğŸš€ Using Ollama at http://localhost:11434 with model 'llama2'

ğŸ“ Prompt: Tell me a short story about network optimization
ğŸ¤– Response (rate-limited):

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Once upon a time, in a digital realm where packets flowed...
[Rate: 45.2 tps, Health: 0.85] ...like rivers through...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… Generated 156 tokens in 3.45s
   Actual rate: 45.2 tps
   Target rate: 45.2 tps
   Network health: 0.85
   Network metrics: RTT=12000us, Retrans=0
```

## ğŸ” æµ‹è¯•ç½‘ç»œæ„ŸçŸ¥æ•ˆæœ

1. **å¯åŠ¨æ•°æ®é‡‡é›†å’Œ Hint Server**
   ```bash
   # ç»ˆç«¯ 1ï¼šæ•°æ®é‡‡é›†
   cd data_collection && sudo bash collect_data.sh
   
   # ç»ˆç«¯ 2ï¼šHint Server
   cd demo && sudo bash run_demo.sh
   ```

2. **æ³¨å…¥ç½‘ç»œæ•…éšœ**ï¼ˆåœ¨å¦ä¸€ä¸ªç»ˆç«¯ï¼‰
   ```bash
   cd data_collection
   python chaos_maker.py --delay 100ms --loss 5%
   ```

3. **è¿è¡ŒçœŸå® LLM å®¢æˆ·ç«¯**
   ```bash
   cd demo
   python real_llm_client.py --engine ollama --prompt "Your prompt here"
   ```

4. **è§‚å¯Ÿé€Ÿç‡å˜åŒ–**ï¼šå½“ç½‘ç»œå‡ºç°æ‹¥å¡æ—¶ï¼ŒToken ç”Ÿæˆé€Ÿç‡ä¼šè‡ªåŠ¨ä¸‹é™

## âš™ï¸ å‘½ä»¤è¡Œå‚æ•°

```bash
python real_llm_client.py --help
```

ä¸»è¦å‚æ•°ï¼š
- `--engine`: é€‰æ‹©å¼•æ“ (`vllm` æˆ– `ollama`)
- `--prompt`: è¾“å…¥æç¤ºè¯
- `--hint-url`: Hint Server URLï¼ˆé»˜è®¤ï¼šhttp://localhost:5000/hintï¼‰
- `--max-tokens`: æœ€å¤§ç”Ÿæˆ Token æ•°
- `--temperature`: é‡‡æ ·æ¸©åº¦
- `--disable-rate-limit`: ç¦ç”¨é€Ÿç‡é™åˆ¶ï¼ˆç”¨äºæµ‹è¯•ï¼‰

vLLM ç‰¹å®šå‚æ•°ï¼š
- `--vllm-url`: vLLM API URLï¼ˆé»˜è®¤ï¼šhttp://localhost:8000/v1ï¼‰
- `--vllm-model`: æ¨¡å‹åç§°

Ollama ç‰¹å®šå‚æ•°ï¼š
- `--ollama-url`: Ollama API URLï¼ˆé»˜è®¤ï¼šhttp://localhost:11434ï¼‰
- `--ollama-model`: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼šllama2ï¼‰

## ğŸ› æ•…éšœæ’é™¤

### Hint Server è¿æ¥å¤±è´¥

```
âš ï¸  Warning: Hint Server may not be running
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç¡®ä¿ Hint Server æ­£åœ¨è¿è¡Œï¼š`cd demo && sudo bash run_demo.sh`
- æˆ–ä½¿ç”¨ `--disable-rate-limit` ç¦ç”¨é€Ÿç‡é™åˆ¶è¿›è¡Œæµ‹è¯•

### Ollama æ¨¡å‹æœªæ‰¾åˆ°

```
âŒ Error: Ollama request failed
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼š`ollama serve`
- ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½ï¼š`ollama pull <model-name>`
- æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®

### vLLM è¿æ¥å¤±è´¥

```
âŒ Error: vLLM request failed
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç¡®ä¿ vLLM æœåŠ¡æ­£åœ¨è¿è¡Œ
- æ£€æŸ¥ç«¯å£æ˜¯å¦æ­£ç¡®ï¼ˆé»˜è®¤ 8000ï¼‰
- æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **æ€§èƒ½å½±å“**ï¼šé€Ÿç‡é™åˆ¶ä¼šåœ¨æ¯ä¸ª Token ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼Œå¯èƒ½ä¼šå½±å“ç”Ÿæˆé€Ÿåº¦
2. **ç½‘ç»œçŠ¶æ€**ï¼šå¦‚æœ Hint Server ä¸å¯ç”¨ï¼Œå®¢æˆ·ç«¯ä¼šä½¿ç”¨é»˜è®¤é€Ÿç‡ï¼ˆ20 tpsï¼‰
3. **å¤šæµæ”¯æŒ**ï¼šå®¢æˆ·ç«¯æ”¯æŒå¤šä¸ªå¹¶å‘æµï¼Œæ¯ä¸ªæµç‹¬ç«‹è·Ÿè¸ªé€Ÿç‡
4. **èµ„æºè¦æ±‚**ï¼švLLM å’Œ Ollama éƒ½éœ€è¦è¶³å¤Ÿçš„ GPU/CPU å’Œå†…å­˜èµ„æº

## ğŸ”— ç›¸å…³æ–‡ä»¶

- `real_llm_client.py`: çœŸå® LLM å®¢æˆ·ç«¯ä¸»ç¨‹åº
- `run_real_llm.sh`: å¯åŠ¨è„šæœ¬
- `hint_server.py`: Hint Serverï¼ˆæä¾›ç½‘ç»œçŠ¶æ€ï¼‰
- `llm_simulator.py`: LLM æ¨¡æ‹Ÿå™¨ï¼ˆç”¨äºå¯¹æ¯”æµ‹è¯•ï¼‰

