# å¤š GPU æ”¯æŒè¯´æ˜

## ğŸ“Š å½“å‰çŠ¶æ€

**å¥½æ¶ˆæ¯**ï¼šeBPF-TokenFlow é¡¹ç›®**å®Œå…¨æ”¯æŒå¤š GPU**ï¼å› ä¸ºï¼š

1. âœ… **vLLM åŸç”Ÿæ”¯æŒå¤š GPU**ï¼švLLM æ”¯æŒ Tensor Parallelism (TP)ã€Pipeline Parallelism (PP) å’Œ Data Parallelism (DP)
2. âœ… **ç½‘ç»œæ„ŸçŸ¥è°ƒåº¦å…¼å®¹å¤š GPU**ï¼šæˆ‘ä»¬çš„å†…æ ¸çº§è°ƒåº¦ç³»ç»Ÿï¼ˆ`health_factor`ï¼‰åœ¨è°ƒåº¦å™¨å±‚é¢å·¥ä½œï¼Œä¸ GPU æ•°é‡æ— å…³
3. âœ… **Hint Server ç‹¬ç«‹è¿è¡Œ**ï¼šHint Server ä¸ä¾èµ– GPUï¼Œå¯ä»¥ç‹¬ç«‹éƒ¨ç½²

## ğŸš€ å¦‚ä½•å¯ç”¨å¤š GPU

### æ–¹æ³• 1: ä½¿ç”¨ Tensor Parallelism (å•èŠ‚ç‚¹å¤š GPU)

**é€‚ç”¨åœºæ™¯**ï¼šæ¨¡å‹å¤ªå¤§ï¼Œå• GPU æ”¾ä¸ä¸‹ï¼Œä½†å¯ä»¥åœ¨å•èŠ‚ç‚¹çš„å¤šä¸ª GPU ä¸Šè¿è¡Œã€‚

```bash
# ä½¿ç”¨ 4 ä¸ª GPU è¿è¡Œï¼ˆTensor Parallelismï¼‰
python3 -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-4B-Instruct-2507 \
    --port 8000 \
    --trust-remote-code \
    --gpu-memory-utilization 0.4 \
    --max-model-len 4096 \
    --tensor-parallel-size 4 \
    --env VLLM_HINT_SERVER_URL=http://localhost:5000/hint
```

**å‚æ•°è¯´æ˜**ï¼š
- `--tensor-parallel-size 4`: ä½¿ç”¨ 4 ä¸ª GPU è¿›è¡Œå¼ é‡å¹¶è¡Œ
- vLLM ä¼šè‡ªåŠ¨å°†æ¨¡å‹åˆ‡åˆ†åˆ°å¤šä¸ª GPU ä¸Š

### æ–¹æ³• 2: ä½¿ç”¨ Pipeline Parallelism (å¤šèŠ‚ç‚¹)

**é€‚ç”¨åœºæ™¯**ï¼šæ¨¡å‹éå¸¸å¤§ï¼Œéœ€è¦è·¨å¤šä¸ªèŠ‚ç‚¹è¿è¡Œã€‚

```bash
# 8 ä¸ª GPU æ€»è®¡ï¼š4 ä¸ª GPU åš Tensor Parallelï¼Œ2 ä¸ª Pipeline é˜¶æ®µ
python3 -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-4B-Instruct-2507 \
    --port 8000 \
    --trust-remote-code \
    --gpu-memory-utilization 0.4 \
    --max-model-len 4096 \
    --tensor-parallel-size 4 \
    --pipeline-parallel-size 2 \
    --env VLLM_HINT_SERVER_URL=http://localhost:5000/hint
```

**å‚æ•°è¯´æ˜**ï¼š
- `--tensor-parallel-size 4`: æ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨ 4 ä¸ª GPU
- `--pipeline-parallel-size 2`: ä½¿ç”¨ 2 ä¸ªèŠ‚ç‚¹ï¼ˆPipeline é˜¶æ®µï¼‰

### æ–¹æ³• 3: ä½¿ç”¨ Data Parallelism (å¤šå‰¯æœ¬)

**é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦æ›´é«˜çš„å¹¶å‘ååé‡ï¼Œè¿è¡Œå¤šä¸ªæ¨¡å‹å‰¯æœ¬ã€‚

```bash
# ä½¿ç”¨ Ray è¿›è¡Œæ•°æ®å¹¶è¡Œï¼ˆéœ€è¦å…ˆå¯åŠ¨ Ray é›†ç¾¤ï¼‰
python3 -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-4B-Instruct-2507 \
    --port 8000 \
    --trust-remote-code \
    --gpu-memory-utilization 0.4 \
    --max-model-len 4096 \
    --data-parallel-size 4 \
    --env VLLM_HINT_SERVER_URL=http://localhost:5000/hint
```

## ğŸ”§ æ›´æ–°å¯åŠ¨è„šæœ¬ä»¥æ”¯æŒå¤š GPU

å¦‚æœä½ æƒ³ä¿®æ”¹å¯åŠ¨è„šæœ¬ä»¥æ”¯æŒå¤š GPUï¼Œå¯ä»¥æ·»åŠ ç¯å¢ƒå˜é‡ï¼š

```bash
# åœ¨å¯åŠ¨è„šæœ¬ä¸­æ·»åŠ 
export VLLM_TENSOR_PARALLEL_SIZE=4  # ä½¿ç”¨ 4 ä¸ª GPU
export VLLM_PIPELINE_PARALLEL_SIZE=1  # å•èŠ‚ç‚¹ï¼ˆä¸éœ€è¦ Pipeline Parallelï¼‰

# ç„¶ååœ¨å¯åŠ¨å‘½ä»¤ä¸­æ·»åŠ 
python3 -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-4B-Instruct-2507 \
    --port 8000 \
    --trust-remote-code \
    --gpu-memory-utilization 0.4 \
    --max-model-len 4096 \
    --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE:-1} \
    --pipeline-parallel-size ${VLLM_PIPELINE_PARALLEL_SIZE:-1} \
    --env VLLM_HINT_SERVER_URL=http://localhost:5000/hint
```

## ğŸ“ˆ æ€§èƒ½å½±å“

### å¤š GPU çš„ä¼˜åŠ¿

1. **æ›´å¤§çš„æ¨¡å‹å®¹é‡**ï¼šå¯ä»¥å°†æ›´å¤§çš„æ¨¡å‹åŠ è½½åˆ°å¤šä¸ª GPU ä¸Š
2. **æ›´é«˜çš„ååé‡**ï¼šTensor Parallelism å¯ä»¥åŠ é€Ÿæ¨ç†
3. **æ›´å¥½çš„å¹¶å‘**ï¼šData Parallelism å¯ä»¥åŒæ—¶å¤„ç†æ›´å¤šè¯·æ±‚

### ç½‘ç»œæ„ŸçŸ¥è°ƒåº¦çš„å…¼å®¹æ€§

âœ… **å®Œå…¨å…¼å®¹**ï¼šæˆ‘ä»¬çš„ `health_factor` è°ƒåº¦æœºåˆ¶åœ¨è°ƒåº¦å™¨å±‚é¢å·¥ä½œï¼Œæ— è®ºä½¿ç”¨å¤šå°‘ä¸ª GPUï¼Œè°ƒåº¦å™¨éƒ½ä¼šæ ¹æ®ç½‘ç»œå¥åº·åº¦è°ƒæ•´ Token é¢„ç®—ã€‚

**å·¥ä½œåŸç†**ï¼š
- è°ƒåº¦å™¨æ ¹æ® `health_factor` é™åˆ¶æ¯è½®çš„ Token é¢„ç®—
- æ— è®ºæ¨¡å‹åˆ†å¸ƒåœ¨å¤šå°‘ä¸ª GPU ä¸Šï¼Œè°ƒåº¦å™¨éƒ½ä¼šç»Ÿä¸€æ§åˆ¶
- å¤š GPU åªæ˜¯åŠ é€Ÿäº†è®¡ç®—ï¼Œä¸å½±å“ç½‘ç»œæ„ŸçŸ¥é€»è¾‘

## ğŸ§ª æµ‹è¯•å¤š GPU é…ç½®

### 1. æ£€æŸ¥ GPU å¯ç”¨æ€§

```bash
# æŸ¥çœ‹å¯ç”¨ GPU
nvidia-smi

# æˆ–è€…
python3 -c "import torch; print(f'GPU æ•°é‡: {torch.cuda.device_count()}')"
```

### 2. æµ‹è¯• Tensor Parallelism

```bash
# ä½¿ç”¨ 2 ä¸ª GPU æµ‹è¯•
python3 -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-4B-Instruct-2507 \
    --port 8000 \
    --trust-remote-code \
    --tensor-parallel-size 2 \
    --env VLLM_HINT_SERVER_URL=http://localhost:5000/hint
```

### 3. éªŒè¯ç½‘ç»œæ„ŸçŸ¥åŠŸèƒ½

```bash
# å¯åŠ¨ Hint Server
python3 demo/hint_server.py

# è¿è¡Œå®¢æˆ·ç«¯æµ‹è¯•
python3 demo/real_llm_client.py \
    --engine vllm \
    --vllm-url http://localhost:8000/v1 \
    --prompt "Test multi-GPU performance"
```

è§‚å¯Ÿè¾“å‡ºä¸­çš„ `Rate` å’Œ `Health` æŒ‡æ ‡ï¼Œåº”è¯¥èƒ½çœ‹åˆ°ç½‘ç»œæ„ŸçŸ¥çš„é€Ÿç‡è°ƒæ•´ã€‚

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. GPU å†…å­˜

- ä½¿ç”¨å¤š GPU æ—¶ï¼Œæ¯ä¸ª GPU çš„å†…å­˜åˆ©ç”¨ç‡ä¼šé™ä½
- å¯ä»¥é€šè¿‡ `--gpu-memory-utilization` è°ƒæ•´
- å»ºè®®ï¼šå¤š GPU æ—¶å¯ä»¥è®¾ç½®æ›´é«˜çš„åˆ©ç”¨ç‡ï¼ˆå¦‚ 0.6-0.8ï¼‰

### 2. é€šä¿¡å¼€é”€

- **Tensor Parallelism**ï¼šGPU ä¹‹é—´éœ€è¦é¢‘ç¹é€šä¿¡ï¼ˆAllReduceï¼‰ï¼Œéœ€è¦ NVLink æˆ–é«˜é€Ÿäº’è¿
- **Pipeline Parallelism**ï¼šèŠ‚ç‚¹ä¹‹é—´éœ€è¦ç½‘ç»œé€šä¿¡ï¼Œéœ€è¦é«˜é€Ÿç½‘ç»œï¼ˆInfiniBand æ¨èï¼‰

### 3. æ¨¡å‹å¤§å°

- å¯¹äº `Qwen/Qwen3-4B-Instruct-2507`ï¼ˆ4B å‚æ•°ï¼‰ï¼Œé€šå¸¸å• GPU å°±è¶³å¤Ÿäº†
- å¦‚æœä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚ 7Bã€13Bã€70Bï¼‰ï¼Œæ‰éœ€è¦å¤š GPU

### 4. CUDA_VISIBLE_DEVICES

å¦‚æœéœ€è¦æŒ‡å®šç‰¹å®šçš„ GPUï¼š

```bash
# åªä½¿ç”¨ GPU 0 å’Œ 1
CUDA_VISIBLE_DEVICES=0,1 python3 -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-4B-Instruct-2507 \
    --tensor-parallel-size 2 \
    ...
```

## ğŸ“š å‚è€ƒèµ„æº

- [vLLM å¹¶è¡ŒåŒ–æ–‡æ¡£](vllm/docs/serving/parallelism_scaling.md)
- [vLLM é…ç½®é€‰é¡¹](https://docs.vllm.ai/en/latest/serving/parallelism.html)
- [Megatron-LM Tensor Parallelism è®ºæ–‡](https://arxiv.org/pdf/1909.08053.pdf)

## ğŸ¯ æ€»ç»“

| ç‰¹æ€§ | å• GPU | å¤š GPU (TP) | å¤š GPU (PP) | å¤š GPU (DP) |
|------|--------|-------------|-------------|-------------|
| **æ¨¡å‹å®¹é‡** | å° | ä¸­ | å¤§ | å°ï¼ˆå¤šå‰¯æœ¬ï¼‰ |
| **æ¨ç†é€Ÿåº¦** | æ…¢ | å¿« | ä¸­ç­‰ | å¿«ï¼ˆå¹¶å‘ï¼‰ |
| **ç½‘ç»œæ„ŸçŸ¥** | âœ… | âœ… | âœ… | âœ… |
| **é€‚ç”¨åœºæ™¯** | å°æ¨¡å‹ | ä¸­ç­‰æ¨¡å‹ | å¤§æ¨¡å‹ | é«˜å¹¶å‘ |

**ç»“è®º**ï¼šeBPF-TokenFlow å®Œå…¨æ”¯æŒå¤š GPUï¼Œä½ åªéœ€è¦åœ¨å¯åŠ¨ vLLM æ—¶æ·»åŠ ç›¸åº”çš„å¹¶è¡Œå‚æ•°å³å¯ï¼

