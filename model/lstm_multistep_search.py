import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import os

# ================= é…ç½®åŒºåŸŸ =================
DATA_PATH = "../data/train_data_congestion.csv" # è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®
INPUT_SEQ_LEN = 10   # è¾“å…¥ï¼šçœ‹è¿‡åŽ» 10 ä¸ªç‚¹ (1.0s)
PRED_SEQ_LEN = 10    # è¾“å‡ºï¼šé¢„æµ‹æœªæ¥ 10 ä¸ªç‚¹ (1.0s)
TEST_SPLIT = 0.2
EPOCHS = 60          # ç¨å¾®å¢žåŠ è½®æ•°ï¼Œè®©å¤§æ¨¡åž‹å……åˆ†æ”¶æ•›
BATCH_SIZE = 32
LR = 0.001

# è¶…å‚æ•°æœç´¢ç©ºé—´
GRID_SEARCH_SPACE = [
    {'hidden_size': 64, 'num_layers': 1},
    {'hidden_size': 128, 'num_layers': 2},
    {'hidden_size': 256, 'num_layers': 2},
]
# ===========================================

def load_data(path):
    print(f"Loading data from {path}...")
    if not os.path.exists(path):
        raise FileNotFoundError(f"Data file not found at {path}")
        
    df = pd.read_csv(path)

    # --- 1. æ•°æ®æ¸…æ´— (å…³é”®ä¼˜åŒ–) ---
    # RTT=0 æ˜¯ç‰©ç†ä¸å¯èƒ½çš„ (é‡‡æ ·ç©ºçª—æœŸ artifacts)ï¼Œä¼šä¸¥é‡è¯¯å¯¼æ¨¡åž‹
    # æˆ‘ä»¬å°†å…¶è§†ä¸ºç¼ºå¤±å€¼ï¼Œå¹¶ç”¨ä¸Šä¸€æ—¶åˆ»çš„æœ‰æ•ˆå€¼å¡«å…… (Forward Fill)
    original_len = len(df)
    df['avg_rtt_us'] = df['avg_rtt_us'].replace(0, np.nan)
    df['avg_rtt_us'] = df['avg_rtt_us'].ffill().bfill() # å…ˆå‰å‘å¡«å……ï¼Œå¼€å¤´å¦‚æžœç¼ºåˆ™åŽå‘å¡«å……
    print(f"Data cleaning: Handled 0-value artifacts in {original_len} rows.")

    df = df.dropna()

    # --- 2. ç‰¹å¾å·¥ç¨‹ ---
    # å¢žåŠ å·®åˆ†ç‰¹å¾ (Gradient)ï¼Œå¸®åŠ©æ¨¡åž‹æ„ŸçŸ¥â€œæ­£åœ¨å˜å¿«â€è¿˜æ˜¯â€œæ­£åœ¨å˜æ…¢â€
    df['rtt_diff'] = df['avg_rtt_us'].diff().fillna(0)
    
    # åŽŸå§‹ç‰¹å¾ç”¨äºŽè¾“å…¥
    feature_cols = [
        'avg_rtt_us', 'p95_rtt_us', 'avg_cwnd', 'throughput_bps',
        'retrans_count', 'rolling_avg_rtt', 'rtt_diff'
    ]
    # é¢„æµ‹ç›®æ ‡ï¼šæœªæ¥çš„ avg_rtt_us
    target_col = 'avg_rtt_us' 

    data_X = df[feature_cols].values
    data_y = df[target_col].values.reshape(-1, 1)
    timestamps = df['timestamp'].values

    # å½’ä¸€åŒ– (Fit on Train, Apply on Test)
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    # Split (æŒ‰æ—¶é—´åˆ‡åˆ†ï¼Œä¸æ‰“ä¹±)
    split_idx = int(len(df) * (1 - TEST_SPLIT))
    
    X_train_raw = data_X[:split_idx]
    X_test_raw = data_X[split_idx:]
    y_train_raw = data_y[:split_idx]
    y_test_raw = data_y[split_idx:]
    
    X_train = scaler_X.fit_transform(X_train_raw)
    X_test = scaler_X.transform(X_test_raw)
    y_train = scaler_y.fit_transform(y_train_raw)
    y_test = scaler_y.transform(y_test_raw)

    return (X_train, y_train), (X_test, y_test), (scaler_X, scaler_y), timestamps[split_idx:]

def create_multistep_sequences(X, y, input_len, pred_len):
    """
    æž„é€  Seq2Seq æ•°æ®:
    Input:  X[t-9 ... t]
    Target: y[t+1 ... t+10]
    """
    xs, ys = [], []
    for i in range(len(X) - input_len - pred_len + 1):
        xs.append(X[i : i + input_len])
        ys.append(y[i + input_len : i + input_len + pred_len])
    
    return np.array(xs), np.array(ys).squeeze()

# ================= æ¨¡åž‹å®šä¹‰ =================
class MultiStepLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_len):
        super(MultiStepLSTM, self).__init__()
        
        # dropouté˜²æ­¢å¤§æ¨¡åž‹è¿‡æ‹Ÿåˆ
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2 if num_layers > 1 else 0)
        
        # Prediction Head
        # å°† LSTM æœ€åŽçš„éšçŠ¶æ€æ˜ å°„ä¸ºæœªæ¥ 10 æ­¥çš„é¢„æµ‹
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 64),
            nn.ReLU(),
            nn.Linear(64, output_len) 
        )

    def forward(self, x):
        # x: [batch, seq_len, features]
        out, _ = self.lstm(x)
        
        # å–æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä½œä¸º Context Vector
        last_step_out = out[:, -1, :] 
        
        # é¢„æµ‹
        predictions = self.fc(last_step_out) 
        return predictions

# ================= è®­ç»ƒä¸Žè¯„ä¼°æµç¨‹ =================
def run_experiment(params, data_pack):
    (X_train, y_train), (X_test, y_test), scalers, _ = data_pack
    input_size = X_train.shape[1]
    
    # æž„é€ åºåˆ—æ•°æ®
    X_train_seq, y_train_seq = create_multistep_sequences(X_train, y_train, INPUT_SEQ_LEN, PRED_SEQ_LEN)
    X_test_seq, y_test_seq = create_multistep_sequences(X_test, y_test, INPUT_SEQ_LEN, PRED_SEQ_LEN)
    
    train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train_seq), torch.FloatTensor(y_train_seq)), 
                              batch_size=BATCH_SIZE, shuffle=True)
    
    model = MultiStepLSTM(input_size, params['hidden_size'], params['num_layers'], PRED_SEQ_LEN)
    
    # ä½¿ç”¨ MSE Loss
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    
    print(f"\nðŸš€ Training Config: {params}")
    model.train()
    for epoch in range(EPOCHS):
        epoch_loss = 0
        for bx, by in train_loader:
            optimizer.zero_grad()
            out = model(bx)
            loss = criterion(out, by)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        
        if (epoch+1) % 10 == 0:
            print(f"  Epoch {epoch+1}/{EPOCHS} | Loss: {epoch_loss/len(train_loader):.4f}")

    # --- è¯„ä¼° ---
    model.eval()
    with torch.no_grad():
        preds_scaled = model(torch.FloatTensor(X_test_seq)).numpy()
    
    # åå½’ä¸€åŒ– [samples, 10]
    preds_real = scalers[1].inverse_transform(preds_scaled.reshape(-1, 1)).reshape(preds_scaled.shape)
    y_true_real = scalers[1].inverse_transform(y_test_seq.reshape(-1, 1)).reshape(y_test_seq.shape)
    
    # è®¡ç®— MAE æŒ‡æ ‡
    mae_overall = mean_absolute_error(y_true_real, preds_real)
    mae_step1 = mean_absolute_error(y_true_real[:, 0], preds_real[:, 0])   # t+0.1s
    mae_step10 = mean_absolute_error(y_true_real[:, -1], preds_real[:, -1]) # t+1.0s
    
    print(f"âœ… Result: Overall MAE={mae_overall:.0f} | Step1 MAE={mae_step1:.0f} | Step10 MAE={mae_step10:.0f}")
    
    return {
        "model": model,
        "params": params,
        "mae_overall": mae_overall,
        "preds": preds_real,
        "truth": y_true_real
    }

# ================= å¯è§†åŒ– =================
def plot_best_result(result, timestamps):
    preds = result['preds']
    truth = result['truth']
    params = result['params']
    
    # é™åˆ¶ç»˜å›¾ç‚¹æ•°ï¼Œé¿å…å¤ªå¯†çœ‹ä¸æ¸…
    limit = 500 
    start_idx = 0
    
    plt.figure(figsize=(15, 8))
    
    # 1. Ground Truth (Step 10 çš„çœŸå®žå€¼)
    # æˆ‘ä»¬ç”»å‡º "Step 10 Truth" å³ t+1.0s æ—¶åˆ»çœŸå®žå‘ç”Ÿçš„ RTT
    plt.plot(range(limit), truth[start_idx:start_idx+limit, 9], color='gray', alpha=0.5, label='Ground Truth (Target at t+1.0s)')
    
    # 2. Step 1 Prediction (çŸ­æœŸé¢„æµ‹ t+0.1s)
    # ä¸ºäº†å¯¹æ¯”ï¼Œæˆ‘ä»¬å°† Step 1 çš„é¢„æµ‹ç”»å‡ºæ¥ï¼ˆé€šå¸¸å®ƒå¾ˆå‡†ï¼Œè´´ç€çœŸå®žå€¼ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç”»çš„æ˜¯ truth[:,0] å¯¹åº”çš„é¢„æµ‹ï¼Œä¸ºäº†è§†è§‰ä¸ä¹±ï¼Œè¿™é‡Œæš‚ä¸ç”» Step 1 çš„çº¿ï¼Œåªç”» Step 10
    
    # 3. Step 10 Prediction (é•¿æœŸé¢„æµ‹ t+1.0s)
    # è¿™æ˜¯æˆ‘ä»¬æœ€å…³å¿ƒçš„ï¼šæ¨¡åž‹åœ¨ t æ—¶åˆ»ï¼Œèƒ½å¦é¢„æµ‹å‡º t+1.0s çš„æ³¢å³°ï¼Ÿ
    plt.plot(range(limit), preds[start_idx:start_idx+limit, 9], color='red', linestyle='--', linewidth=1.5,
             label=f'LSTM Pred (t+1.0s) - MAE: {mean_absolute_error(truth[:,9], preds[:,9]):.0f}')

    plt.title(f"Best Model: {params}\nTask: Predict RTT 1.0s into the future (Step 10)")
    plt.xlabel("Time Steps (0.1s units)")
    plt.ylabel("RTT (us)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig("lstm_optimized_result.png")
    print("\nðŸ“Š Graph saved to lstm_optimized_result.png")
    plt.show()

# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    try:
        data_pack = load_data(DATA_PATH)
        (_, _, _, timestamps) = data_pack
        
        best_mae = float('inf')
        best_result = None
        
        print(">>> Starting Hyperparameter Search for Multi-step Prediction...")
        
        for params in GRID_SEARCH_SPACE:
            res = run_experiment(params, data_pack)
            if res['mae_overall'] < best_mae:
                best_mae = res['mae_overall']
                best_result = res
                
                # ä¿å­˜æœ€ä½³æ¨¡åž‹
                torch.save(res['model'].state_dict(), "best_lstm_multistep.pth")
                print("ðŸ’¾ Model saved to best_lstm_multistep.pth")
                
        print("\nðŸ† All experiments done.")
        print(f"Best Params: {best_result['params']} with MAE: {best_result['mae_overall']:.2f}")
        
        plot_best_result(best_result, timestamps)
        
    except Exception as e:
        print(f"âŒ Error: {e}")