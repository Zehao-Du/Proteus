import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import os
import itertools
import random
import time

# ================= ğŸ” æœç´¢ç©ºé—´é…ç½® =================
# åœ¨è¿™é‡Œå®šä¹‰ä½ æƒ³å°è¯•çš„æ‰€æœ‰ç»„åˆ
SEARCH_SPACE = {
    'learning_rate': [0.01, 0.005, 0.002, 0.001, 0.0005],
    'epochs':        [50, 100, 150, 200],
    'batch_size':    [512]  # Batch Size ä¹Ÿä¼šå½±å“æ”¶æ•›
}

# å›ºå®šå‚æ•°
DATA_PATH = "../data/train_data_congestion.csv"
INPUT_SEQ_LEN = 10
PRED_SEQ_LEN = 10
TEST_SPLIT = 0.2
HIDDEN_SIZE = 256
NUM_LAYERS = 2
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ==================================================

print(f"ğŸ–¥ï¸  Running on device: {DEVICE}")

# --- è¾…åŠ©åŠŸèƒ½ ---
def set_seed(seed=42):
    """å›ºå®šéšæœºç§å­ï¼Œä¿è¯æ¯æ¬¡å®éªŒèµ·ç‚¹ä¸€è‡´ï¼Œå…¬å¹³æ¯”è¾ƒ"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

class AsymmetricMSELoss(nn.Module):
    def __init__(self, penalty=10.0): 
        super().__init__()
        self.penalty = penalty

    def forward(self, pred, target):
        error = target - pred
        loss = torch.where(error > 0, error**2 * self.penalty, error**2)
        return torch.mean(loss)

def load_data(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Data file not found at {path}")
    df = pd.read_csv(path)
    
    # æ¸…æ´—ä¸ç‰¹å¾å·¥ç¨‹
    df['avg_rtt_us'] = df['avg_rtt_us'].replace(0, np.nan)
    df.loc[df['avg_rtt_us'] < 10, 'avg_rtt_us'] = np.nan 
    df['avg_rtt_us'] = df['avg_rtt_us'].ffill().bfill()
    df['log_rtt'] = np.log1p(df['avg_rtt_us']) 
    df['rtt_diff'] = df['log_rtt'].diff().fillna(0) 
    
    feature_cols = ['log_rtt', 'p95_rtt_us', 'avg_cwnd', 'throughput_bps', 'retrans_count', 'rolling_avg_rtt', 'rtt_diff']
    target_col = 'log_rtt'

    data_X = df[feature_cols].values
    data_y = df[target_col].values.reshape(-1, 1)

    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    
    split_idx = int(len(df) * (1 - TEST_SPLIT))
    
    X_train = scaler_X.fit_transform(data_X[:split_idx])
    X_test = scaler_X.transform(data_X[split_idx:])
    y_train = scaler_y.fit_transform(data_y[:split_idx])
    y_test = scaler_y.transform(data_y[split_idx:])

    return (X_train, y_train), (X_test, y_test), (scaler_X, scaler_y)

def create_sequences(X, y, input_len, pred_len):
    xs, ys = [], []
    for i in range(len(X) - input_len - pred_len + 1):
        xs.append(X[i : i + input_len])
        ys.append(y[i + input_len : i + input_len + pred_len])
    return np.array(xs), np.array(ys).squeeze()

class MultiStepLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_len):
        super(MultiStepLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 128),
            nn.ReLU(),
            nn.Linear(128, output_len) 
        )

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

# --- æ ¸å¿ƒè®­ç»ƒå‡½æ•° ---
def train_and_evaluate(params, data_pack):
    """
    è¿è¡Œä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒå¹¶è¿”å›æŒ‡æ ‡
    params: dict {'lr': ..., 'epochs': ..., 'batch_size': ...}
    """
    # 1. è§£åŒ…æ•°æ®
    (X_train, y_train), (X_test, y_test), scalers = data_pack
    input_size = X_train.shape[1]

    # 2. å‡†å¤‡ Loader
    X_train_seq, y_train_seq = create_sequences(X_train, y_train, INPUT_SEQ_LEN, PRED_SEQ_LEN)
    X_test_seq, y_test_seq = create_sequences(X_test, y_test, INPUT_SEQ_LEN, PRED_SEQ_LEN)
    
    train_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train_seq), torch.FloatTensor(y_train_seq)), 
                              batch_size=params['batch_size'], shuffle=True)
    
    # 3. åˆå§‹åŒ–æ¨¡å‹ (æ¯æ¬¡éƒ½è¦å…¨æ–°çš„)
    set_seed(42) # é‡è¦ï¼é‡ç½®ç§å­
    model = MultiStepLSTM(input_size, HIDDEN_SIZE, NUM_LAYERS, PRED_SEQ_LEN).to(DEVICE)
    criterion = AsymmetricMSELoss(penalty=10.0).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])

    # 4. è®­ç»ƒå¾ªç¯
    model.train()
    start_time = time.time()
    for epoch in range(params['epochs']):
        for bx, by in train_loader:
            bx, by = bx.to(DEVICE), by.to(DEVICE)
            optimizer.zero_grad()
            out = model(bx)
            loss = criterion(out, by)
            loss.backward()
            optimizer.step()
    
    duration = time.time() - start_time

    # 5. è¯„ä¼°
    model.eval()
    with torch.no_grad():
        test_x_tensor = torch.FloatTensor(X_test_seq).to(DEVICE)
        preds_scaled = model(test_x_tensor).cpu().numpy()

    # 6. åå½’ä¸€åŒ– & æŒ‡æ ‡è®¡ç®—
    preds_log = scalers[1].inverse_transform(preds_scaled).reshape(preds_scaled.shape)
    y_true_log = scalers[1].inverse_transform(y_test_seq).reshape(y_test_seq.shape)
    
    preds_real = np.expm1(preds_log)
    y_true_real = np.expm1(y_true_log)

    # å…³æ³¨ Step 10
    step10_truth = y_true_real[:, -1]
    step10_pred = preds_real[:, -1]
    
    mae_total = mean_absolute_error(step10_truth, step10_pred)
    
    residuals = step10_truth - step10_pred
    under_errors = residuals[residuals > 0]
    mae_under = np.mean(under_errors) if len(under_errors) > 0 else 0

    return {
        'mae': mae_total,
        'mae_under': mae_under,
        'duration': duration,
        'model_state': model.state_dict() # è¿”å›æƒé‡ä»¥ä¾¿ä¿å­˜
    }

# ================= ä¸»æ§åˆ¶æµç¨‹ =================
if __name__ == "__main__":
    # 1. åŠ è½½æ•°æ® (åªåšä¸€æ¬¡)
    print("ğŸ“¥ Loading Data...")
    data_pack = load_data(DATA_PATH)
    
    # 2. ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
    keys, values = zip(*SEARCH_SPACE.items())
    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]
    
    print(f"ğŸ” Starting Grid Search over {len(param_combinations)} combinations...")
    print("-" * 60)
    print(f"{'LR':<10} | {'Epochs':<8} | {'Batch':<6} | {'MAE (us)':<10} | {'Danger Err':<10} | {'Time (s)':<8}")
    print("-" * 60)
    
    results = []
    best_score = float('inf')
    
    # 3. å¼€å§‹å¾ªç¯
    for params in param_combinations:
        try:
            # è¿è¡Œå®éªŒ
            res = train_and_evaluate(params, data_pack)
            
            # æ‰“å°ç»“æœ
            print(f"{params['learning_rate']:<10} | {params['epochs']:<8} | {params['batch_size']:<6} | "
                  f"{res['mae']:<10.0f} | {res['mae_under']:<10.0f} | {res['duration']:<8.1f}")
            
            # è®°å½•
            results.append({
                **params,
                'mae': res['mae'],
                'mae_under': res['mae_under']
            })
            
            # 4. è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ (ä»¥ Dangerous Error ä¸ºå‡†ï¼Œè¿˜æ˜¯ä»¥ Total MAE ä¸ºå‡†ï¼Ÿ)
            # è¿™é‡Œæˆ‘é€‰æ‹©ç»¼åˆæŒ‡æ ‡ï¼šMAE ä¸èƒ½å¤ªå·®ï¼Œä½† Danger è¦å°½å¯èƒ½å°
            # ç®€å•èµ·è§ï¼Œè¿™é‡Œä»¥ Dangerous Error ä¸ºç¬¬ä¸€ä¼˜åŒ–ç›®æ ‡ (å› ä¸ºæ˜¯æ‹¥å¡æ§åˆ¶)
            current_score = res['mae_under'] 
            
            if current_score < best_score:
                best_score = current_score
                torch.save(res['model_state'], "best_lstm_grid_search.pth")
                print(f"   ğŸŒŸ New Best Found! Model saved.")
                
            # æ¸…ç†æ˜¾å­˜
            del res['model_state']
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ Error with params {params}: {e}")

    # 5. æ€»ç»“æŠ¥å‘Š
    print("\n" + "="*40)
    print("ğŸ† Grid Search Top 5 Results")
    print("="*40)
    
    # è½¬ä¸º DataFrame æ–¹ä¾¿æ’åº
    df_res = pd.DataFrame(results)
    # æŒ‰ 'mae_under' (ä½ä¼°è¯¯å·®) å‡åºæ’åˆ—
    df_sorted = df_res.sort_values(by='mae_under')
    
    print(df_sorted.head(5).to_string(index=False))
    
    print(f"\nğŸ’¾ Best model saved to: best_lstm_grid_search.pth")
    print(f"   Best Params: {df_sorted.iloc[0].to_dict()}")