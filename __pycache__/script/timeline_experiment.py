#!/usr/bin/env python3
"""
ğŸ• Timeline Experiment - éªŒè¯ Network-Aware è°ƒåº¦çš„çœŸæ­£ä¼˜åŠ¿

æ ¸å¿ƒæ´å¯Ÿï¼š
- GPU ç”Ÿæˆé€Ÿåº¦å›ºå®šï¼Œä½† chunk åˆ°è¾¾å®¢æˆ·ç«¯çš„æ—¶é—´ä¸åŒ
- ç½‘ç»œå¥½ç”¨æˆ·ï¼šchunk ç«‹å³åˆ°è¾¾
- ç½‘ç»œå·®ç”¨æˆ·ï¼šchunk å»¶è¿Ÿåˆ°è¾¾
- Network-aware ä¼˜å…ˆç½‘ç»œå¥½ç”¨æˆ· â†’ ç´¯è®¡æœ‰æ•ˆ chunk æ›²çº¿ä¸€ç›´åœ¨ä¸Šé¢

è¾“å‡ºï¼šç´¯è®¡æœ‰æ•ˆ chunk éšæ—¶é—´å˜åŒ–çš„æ›²çº¿å›¾
"""

import argparse
import asyncio
import aiohttp
import json
import time
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass, field
from typing import List, Dict, Tuple
from collections import defaultdict
import sys


@dataclass
class UserProfile:
    """ç”¨æˆ·é…ç½®"""
    user_id: int
    rtt: float  # msï¼Œå½±å“ chunk åˆ°è¾¾æ—¶é—´
    health: float  # 0.0-1.0
    category: str  # 'very_bad', 'bad', 'good', 'very_good'


@dataclass
class ChunkEvent:
    """Chunk äº‹ä»¶ï¼šè®°å½•æ¯ä¸ª SSE chunk çš„è§‚æµ‹æ—¶é—´å’Œåˆæˆåˆ°è¾¾æ—¶é—´
    
    æ³¨æ„ï¼šobserved_arrival_time æ˜¯å®¢æˆ·ç«¯å®é™…æ”¶åˆ° SSE chunk çš„æ—¶é—´
    ï¼ˆåœ¨ localhost åœºæ™¯ä¸‹ï¼Œè¿‘ä¼¼ç­‰äº GPU ç”Ÿæˆæ—¶é—´ï¼‰
    synthetic_arrival_time æ˜¯åŠ å…¥ç½‘ç»œ RTT å»¶è¿Ÿåçš„"æœ‰æ•ˆåˆ°è¾¾æ—¶é—´"
    """
    user_id: int
    chunk_idx: int
    observed_arrival_time: float  # å®¢æˆ·ç«¯è§‚æµ‹åˆ°çš„åˆ°è¾¾æ—¶é—´ï¼ˆâ‰ˆ GPU ç”Ÿæˆæ—¶é—´ï¼Œlocalhostï¼‰
    synthetic_arrival_time: float  # åŠ å…¥ RTT å»¶è¿Ÿåçš„åˆæˆåˆ°è¾¾æ—¶é—´ï¼ˆç”¨äºè®¡ç®—æœ‰æ•ˆååï¼‰
    rtt: float
    category: str
    chunk_length: int  # chunk å†…å®¹é•¿åº¦


@dataclass
class RequestStats:
    """è¯·æ±‚ç»Ÿè®¡ï¼šè®°å½•æ¯ä¸ªè¯·æ±‚çš„å…³é”®æŒ‡æ ‡"""
    user_id: int
    category: str
    rtt: float
    profile_health: float  # ç”¨æˆ·é…ç½®çš„å¥åº·åº¦ï¼ˆprofile.healthï¼‰
    used_health_factor: float  # å®é™…ä½¿ç”¨çš„å¥åº·åº¦ï¼ˆbaseline=1.0, network-aware=profile.healthï¼‰
    ttft: float  # Time To First Chunkï¼ˆç§’ï¼‰
    total_chunks: int
    total_time: float  # è¯·æ±‚æ€»æ—¶é—´ï¼ˆç§’ï¼‰


def generate_user_profiles(num_users: int = 8192) -> List[UserProfile]:
    """ç”Ÿæˆç”¨æˆ·é…ç½® - ä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼ˆæœ‰é•¿å°¾ï¼‰
    
    RTT åˆ†å¸ƒï¼š
    - å‡å€¼ 400msï¼Œæ ‡å‡†å·® 1000ms
    - æˆªæ–­åˆ° [0, 800000] ms èŒƒå›´ï¼ˆæ¨¡æ‹Ÿæç«¯æƒ…å†µï¼šæœ‰äºº RTT å¾ˆå°ï¼Œæœ‰äººå¾ˆå¤§ï¼‰
    - å½¢æˆé•¿å°¾åˆ†å¸ƒï¼šå¤§å¤šæ•°ç”¨æˆ·ç½‘ç»œæ­£å¸¸ï¼Œå°‘æ•°ç”¨æˆ·ç½‘ç»œå¾ˆå·®
    
    å…³é”®ï¼šä½¿ç”¨ user_id ä½œä¸ºç§å­ï¼Œç¡®ä¿ä¸ Hint Server ä¸€è‡´ï¼
    """
    profiles = []
    
    for user_id in range(1, num_users + 1):
        # ä½¿ç”¨ user_id ä½œä¸ºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§
        # è¿™æ ·æ— è®ºåœ¨å“ªé‡Œè°ƒç”¨ï¼Œuser_id=N çš„ RTT éƒ½ç›¸åŒ
        np.random.seed(user_id + 42)  # +42 ä½œä¸ºåç§»
        
        # æ­£æ€åˆ†å¸ƒ RTTï¼šmean=400ms, std=1000ms
        rtt = np.random.normal(loc=400, scale=1000)
        rtt = float(np.clip(rtt, 0, 800000))
        
        # æ ¹æ® RTT è®¡ç®—å¥åº·åº¦ï¼šhealth = exp(-RTT / 500)
        # ä½¿ç”¨ 500 è€Œä¸æ˜¯ 150ï¼Œä»¥é€‚é…æ–°çš„ RTT åˆ†å¸ƒ (loc=400, scale=1000)
        health = float(np.exp(-rtt / 500.0))
        
        # æ ¹æ® RTT åˆ†ç±»
        if rtt >= 400:
            category = 'very_bad'
        elif rtt >= 200:
            category = 'bad'
        elif rtt >= 80:
            category = 'good'
        else:
            category = 'very_good'
        
        profiles.append(UserProfile(
            user_id=user_id,
            rtt=rtt,
            health=health,
            category=category
        ))
    
    # æ‰“ä¹±å‘é€é¡ºåºï¼Œä½† user_id â†’ RTT çš„æ˜ å°„ä¿æŒä¸å˜
    np.random.seed(999)
    np.random.shuffle(profiles)
    return profiles

def generate_user_profiles_multimodal(num_users: int = 8192) -> List[UserProfile]:
    """æ”¹è¿›ç‰ˆï¼šä½¿ç”¨æ··åˆé«˜æ–¯åˆ†å¸ƒæ¨¡æ‹ŸçœŸå®çš„ 4 ç±»ç”¨æˆ·ç¾¤ä½“"""
    profiles = []
    
    # å®šä¹‰ 4 ç§ç½‘ç»œç¯å¢ƒçš„å‚æ•° (æ¦‚ç‡, å‡å€¼ms, æ ‡å‡†å·®ms, ç±»åˆ«å)
    # æ¦‚ç‡æ€»å’Œåº”ä¸º 1.0
    network_clusters = [
        # 1. æå¥½ç½‘ç»œ (å…‰çº¤/åŒåŸ): æå‡åˆ° 50%
        {'prob': 0.50, 'loc': 20,  'scale': 10,  'cat': 'very_good'},
        
        # 2. æ™®é€šç½‘ç»œ (4G/Wi-Fi): æå‡åˆ° 40%
        {'prob': 0.40, 'loc': 200, 'scale': 30,  'cat': 'good'},
        
        # 3. è¾ƒå·®ç½‘ç»œ (è·¨å›½): é™åˆ° 9% (ä½œä¸ºè¾¹ç¼˜æ¡ˆä¾‹)
        {'prob': 0.09, 'loc': 700, 'scale': 80,  'cat': 'bad'},
        
        # 4. æå·®ç½‘ç»œ (å«æ˜Ÿ): é™åˆ° 1% (ä½œä¸ºæç«¯æ¡ˆä¾‹)
        {'prob': 0.01, 'loc': 2000,'scale': 400, 'cat': 'very_bad'}
    ]
    # network_clusters = [
    #     # 1. æå¥½ç½‘ç»œ (å…‰çº¤/åŒåŸ): çº¦å  40%
    #     {'prob': 0.20, 'loc': 20,  'scale': 10,  'cat': 'very_good'},
        
    #     # 2. æ™®é€šç½‘ç»œ (4G/Wi-Fi): çº¦å  30%
    #     {'prob': 0.70, 'loc': 200, 'scale': 30,  'cat': 'good'},
        
    #     # 3. è¾ƒå·®ç½‘ç»œ (è·¨å›½/æ‹¥å µ): çº¦å  20%
    #     {'prob': 0.05, 'loc': 700, 'scale': 80,  'cat': 'bad'},
        
    #     # 4. æå·®ç½‘ç»œ (å¼±ä¿¡å·/å«æ˜Ÿ): çº¦å  10%
    #     {'prob': 0.05, 'loc': 2000,'scale': 400, 'cat': 'very_bad'}
    # ]
    
    for user_id in range(1, num_users + 1):
        np.random.seed(user_id + 42)
        
        # 1. å…ˆå†³å®šè¿™ä¸ªç”¨æˆ·å±äºå“ªä¸ªç¾¤ä½“
        cluster_idx = np.random.choice(
            len(network_clusters), 
            p=[c['prob'] for c in network_clusters]
        )
        cluster = network_clusters[cluster_idx]
        
        # 2. åœ¨è¯¥ç¾¤ä½“çš„åˆ†å¸ƒå†…ç”Ÿæˆ RTT
        rtt = np.random.normal(loc=cluster['loc'], scale=cluster['scale'])
        
        # 3. ç‰©ç†é™åˆ¶ä¿®æ­£ï¼ˆRTT ä¸èƒ½å°äº 5msï¼Œä¸èƒ½æ— é™å¤§ï¼‰
        rtt = float(np.clip(rtt, 5, 10000))
        
        # 4. è®¡ç®—å¥åº·åº¦ (æ²¿ç”¨åŸé€»è¾‘)
        # æ³¨æ„ï¼šè¿™é‡Œ health è®¡ç®—å¯èƒ½éœ€è¦æ ¹æ®ä¸åŒç¾¤ä½“çš„ RTT èŒƒå›´åšå¾®è°ƒï¼Œ
        # æˆ–è€…ç»§ç»­ä½¿ç”¨ç»Ÿä¸€çš„è¡°å‡å…¬å¼
        health = float(np.exp(-rtt / 500.0))
        
        profiles.append(UserProfile(
            user_id=user_id,
            rtt=rtt,
            health=health,
            category=cluster['cat'] # ç›´æ¥ä½¿ç”¨ç”Ÿæˆçš„ç±»åˆ«
        ))
    
    # æ‰“ä¹±é¡ºåºï¼Œæ¨¡æ‹ŸçœŸå®åˆ°è¾¾
    np.random.seed(999)
    np.random.shuffle(profiles)
    return profiles

class TimelineExperiment:
    """æ—¶é—´çº¿å®éªŒ"""
    
    def __init__(
        self,
        vllm_url: str = "http://localhost:8000/v1",
        num_users: int = 1024,
        max_tokens: int = 50,
        concurrency: int = 256,
        client_concurrency: int = 2048,
        target_qps: float = 20.0,
    ):
        self.vllm_url = vllm_url
        self.num_users = num_users
        self.max_tokens = max_tokens
        self.concurrency = concurrency  # vLLM çš„ max_num_seqs
        self.client_concurrency = client_concurrency  # å®¢æˆ·ç«¯å¹¶å‘è¿æ¥æ•°
        self.target_qps = target_qps
        self.model_name = None
        
        # ç”Ÿæˆç”¨æˆ·é…ç½®ï¼ˆä½¿ç”¨å›ºå®šç§å­ä¿è¯ä¸¤æ¬¡å®éªŒç”¨æˆ·ç›¸åŒï¼‰
        np.random.seed(42)
        self.user_profiles = generate_user_profiles_multimodal(num_users)
        
    async def detect_model(self, session: aiohttp.ClientSession) -> str:
        """æ£€æµ‹æ¨¡å‹åç§°"""
        try:
            async with session.get(f"{self.vllm_url}/models") as resp:
                if resp.status == 200:
                    data = await resp.json()
                    models = data.get("data", [])
                    if models:
                        return models[0].get("id", "unknown")
        except:
            pass
        return "unknown"
    
    async def send_request(
        self,
        session: aiohttp.ClientSession,
        profile: UserProfile,
        experiment_start_time: float,
        semaphore: asyncio.Semaphore,
        mode: str,
    ) -> Tuple[List[ChunkEvent], RequestStats]:
        """å‘é€å•ä¸ªè¯·æ±‚ï¼Œè®°å½•æ¯ä¸ª chunk çš„äº‹ä»¶å’Œ TTFC
        
        ä¿®æ”¹ç‚¹ï¼š
        1. æ¨¡æ‹Ÿä¸Šè¡Œå»¶è¿Ÿï¼šè¯·æ±‚å‘é€å‰ sleep (RTT/2 + æƒ©ç½š)
        2. æ¨¡æ‹Ÿä¸‹è¡Œå»¶è¿Ÿï¼šæ¥æ”¶æ•°æ®å add (RTT/2 + æƒ©ç½š)
        """
        events = []
        # è®°å½•è¯·æ±‚å¼€å§‹å¤„ç†çš„æ—¶é—´ï¼ˆClient å†³å®šå‘é€çš„æ—¶é—´ï¼‰
        request_start_time = time.perf_counter()
        first_chunk_time = None
        
        prompt = f"User {profile.user_id}: Write a brief story about AI."
        
        import uuid
        custom_request_id = f"user{profile.user_id}_{uuid.uuid4().hex[:8]}"
        
        # ------------------------------------------------------------------
        # 1. ä¼˜å…ˆçº§/å¥åº·åº¦è®¡ç®—
        # ------------------------------------------------------------------
        if mode == "baseline":
            health_factor = 1.0
        else:
            # health_factor = 1.0 - profile.health 
            health_factor = profile.health # åŸç‰ˆ
            
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": self.max_tokens,
            "stream": True,
            "temperature": 0.0,
            "top_p": 1.0,
            "ignore_eos": True,
            "user": f"user{profile.user_id}",
            "request_id": custom_request_id,
            "vllm_xargs": {
                "health_factor": health_factor
            }
        }
        
        # ------------------------------------------------------------------
        # 2. è®¡ç®—å•å‘å»¶è¿Ÿ (One-Way Delay)
        # ------------------------------------------------------------------
        # é€»è¾‘ï¼šç‰©ç†ä¼ è¾“æ—¶é—´(RTT/2) + æ‹¥å¡æƒ©ç½š(RTT^2 çš„ä¸€åŠ)
        # è¿™æ · ä¸Šè¡Œ+ä¸‹è¡Œ çš„æ€»å»¶è¿Ÿ â‰ˆ RTT + RTT^2
        rtt_sec = profile.rtt / 1000.0
        one_way_delay = (rtt_sec / 2.0) + (0.5 * (rtt_sec ** 2))
        
        async with semaphore:
            # --------------------------------------------------------------
            # 3. æ¨¡æ‹Ÿä¸Šè¡Œå»¶è¿Ÿ (Uplink Latency)
            # --------------------------------------------------------------
            # è¯·æ±‚åœ¨è·¯ä¸Šè·‘ï¼Œè¿˜æ²¡åˆ° vLLM
            await asyncio.sleep(one_way_delay)
            
            try:
                # è¿™é‡Œçš„ session.post å‘ç”Ÿæ—¶åˆ»ï¼Œç›¸å½“äº Server æ”¶åˆ°è¯·æ±‚çš„æ—¶åˆ»
                async with session.post(
                    f"{self.vllm_url}/chat/completions",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=300),
                ) as resp:
                    if resp.status != 200:
                        return events, None
                    
                    chunk_idx = 0
                    
                    async for line in resp.content:
                        if not line:
                            continue
                        
                        line_str = line.decode('utf-8').strip()
                        if not line_str.startswith("data: "):
                            continue
                        
                        data_str = line_str[6:]
                        if data_str == "[DONE]":
                            break
                        
                        try:
                            data = json.loads(data_str)
                            choices = data.get("choices", [])
                            if choices:
                                delta = choices[0].get("delta", {})
                                content = delta.get("content")
                                if content:
                                    current_time = time.perf_counter()
                                    
                                    # TTFC è®¡ç®—ï¼šåŒ…å«äº† ä¸Šè¡Œå»¶è¿Ÿ + æ’é˜Ÿ + ç”Ÿæˆ + ç½‘ç»œä¼ è¾“(å¦‚æœæ˜¯çœŸå®ç½‘ç»œ)
                                    if first_chunk_time is None:
                                        first_chunk_time = current_time - request_start_time
                                    
                                    # -------------------------------------------------------
                                    # 4. è§‚æµ‹æ—¶é—´ & ä¸‹è¡Œå»¶è¿Ÿ
                                    # -------------------------------------------------------
                                    # observed: å®é™…ä¸Šå› ä¸ºå‰é¢ sleep äº†ä¸Šè¡Œæ—¶é—´ï¼Œ
                                    # è¿™ä¸ª observed æ—¶é—´å·²ç»åŒ…å«äº† (ä¸Šè¡Œ + GPUå¤„ç†)
                                    observed_arrival_time = current_time - experiment_start_time
                                    
                                    # synthetic: åœ¨ observed åŸºç¡€ä¸Šå†åŠ ä¸€æ®µå›å»çš„è·¯ç¨‹ (ä¸‹è¡Œ)
                                    synthetic_arrival_time = observed_arrival_time + one_way_delay
                                    
                                    events.append(ChunkEvent(
                                        user_id=profile.user_id,
                                        chunk_idx=chunk_idx,
                                        observed_arrival_time=observed_arrival_time,
                                        synthetic_arrival_time=synthetic_arrival_time,
                                        rtt=profile.rtt,
                                        category=profile.category,
                                        chunk_length=len(content)
                                    ))
                                    chunk_idx += 1
                        except json.JSONDecodeError:
                            continue
            except Exception as e:
                pass
        
        # åˆ›å»ºè¯·æ±‚ç»Ÿè®¡
        total_time = time.perf_counter() - request_start_time
        used_health_factor = 1.0 if mode == "baseline" else profile.health
        
        stats = RequestStats(
            user_id=profile.user_id,
            category=profile.category,
            rtt=profile.rtt,
            profile_health=profile.health,
            used_health_factor=used_health_factor,
            ttft=first_chunk_time if first_chunk_time else 0,
            total_chunks=len(events),
            total_time=total_time
        ) if events else None
        
        return events, stats
    
    async def run_experiment(
        self, 
        mode: str, 
        shuffled_profiles: List[UserProfile]
    ) -> Tuple[List[ChunkEvent], List[RequestStats], float]:
        """è¿è¡Œå®éªŒï¼Œè¿”å›æ‰€æœ‰ chunk äº‹ä»¶å’Œè¯·æ±‚ç»Ÿè®¡
        
        Args:
            mode: "baseline" æˆ– "network_aware"
            shuffled_profiles: å·²æ‰“ä¹±çš„ç”¨æˆ·é…ç½®åˆ—è¡¨ï¼ˆä¸¤ä¸ªå®éªŒå¤ç”¨åŒä¸€ä¸ªé¡ºåºï¼‰
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ Running {mode.upper()} - {self.num_users} Users")
        print(f"{'='*60}")
        
        # è®¾ç½® TCPConnector limitï¼Œé¿å… aiohttp è‡ªå·±æ— é™å¼€è¿æ¥
        connector = aiohttp.TCPConnector(limit=self.client_concurrency)
        async with aiohttp.ClientSession(connector=connector) as session:
            if not self.model_name:
                self.model_name = await self.detect_model(session)
                print(f"ğŸ“¦ Model: {self.model_name}")
            
            print(f"ğŸ”¢ Users: {self.num_users}")
            print(f"ğŸ¯ vLLM max_num_seqs (assumed): {self.concurrency}")
            print(f"ğŸ”Œ Client concurrency: {self.client_concurrency}")
            print(f"ğŸ“ Max tokens: {self.max_tokens}")
            print(f"âš ï¸  Note: max_num_seqs must match vLLM server config")
            
            experiment_start_time = time.perf_counter()
            
            # ä½¿ç”¨ Semaphore é™åˆ¶å®¢æˆ·ç«¯å¹¶å‘è¿æ¥æ•°
            # ç­–ç•¥ï¼šè¶³å¤Ÿå¤§ä½†æœ‰é™ï¼Œè®© backlog è¿›å…¥ vLLM çš„ waiting é˜Ÿåˆ—
            # ä½†ä¸ä¼šå‹å®ç³»ç»Ÿï¼ˆé¿å…è¿æ¥é£æš´å’Œ IO ç“¶é¢ˆï¼‰
            semaphore = asyncio.Semaphore(self.client_concurrency)
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡ï¼ˆä½†å— Semaphore æ§åˆ¶å¹¶å‘æ‰§è¡Œï¼‰
            tasks = [
                self.send_request(session, profile, experiment_start_time, semaphore, mode)
                for profile in shuffled_profiles
            ]
            
            # è®¡ç®—é¢„æœŸçš„ waiting é˜Ÿåˆ—è§„æ¨¡
            # ç”±äº Semaphore é™åˆ¶ï¼ŒåŒä¸€æ—¶åˆ»æœ€å¤š client_concurrency ä¸ªè¯·æ±‚è¿›å…¥ vLLM
            # å…¶ä¸­ concurrency ä¸ªåœ¨ runningï¼Œå…¶ä½™åœ¨ waiting
            expected_backlog = max(0, self.client_concurrency - self.concurrency)
            
            print(f"\nâ³ Sending {len(tasks)} requests...")
            print(f"   Client concurrency: {self.client_concurrency} (Semaphore)")
            print(f"   vLLM max_num_seqs: {self.concurrency} (scheduler limit)")
            print(f"   Expected backlog: ~{expected_backlog} requests in waiting queue")
            print(f"   Health factor: {'1.0 (all users)' if mode == 'baseline' else 'varies by RTT'}")
            results = await asyncio.gather(*tasks)
            
            duration = time.perf_counter() - experiment_start_time
        
        # åˆå¹¶æ‰€æœ‰äº‹ä»¶å’Œç»Ÿè®¡
        all_events = []
        all_stats = []
        completed_requests = 0
        failed_requests = 0
        for events, stats in results:
            if stats and len(events) > 0:
                all_events.extend(events)
                all_stats.append(stats)
                completed_requests += 1
            else:
                failed_requests += 1
        
        # è®¡ç®— TTFC ç»Ÿè®¡
        if all_stats:
            ttfts = [s.ttft for s in all_stats if s.ttft > 0]  # ttft å­—æ®µåä¿ç•™ï¼Œä½†å®é™…æ˜¯ TTFC
            if ttfts:
                avg_ttft = np.mean(ttfts)
                p50_ttft = np.percentile(ttfts, 50)
                p95_ttft = np.percentile(ttfts, 95)
                p99_ttft = np.percentile(ttfts, 99)
                
                # Chunk é•¿åº¦éªŒè¯ï¼ˆæ£€æŸ¥æ˜¯å¦ä¸€ token ä¸€ chunkï¼‰
                chunk_lengths = [e.chunk_length for e in all_events]
                if chunk_lengths:
                    avg_chunk_len = np.mean(chunk_lengths)
                    max_chunk_len = max(chunk_lengths)
                    chunk_len_dist = {1: sum(1 for l in chunk_lengths if l == 1),
                                    2: sum(1 for l in chunk_lengths if l == 2),
                                    3: sum(1 for l in chunk_lengths if l == 3),
                                    '>3': sum(1 for l in chunk_lengths if l > 3)}
                
                print(f"ğŸ“Š Total chunks: {len(all_events)}")
                print(f"âœ… Completed requests: {completed_requests}/{len(results)}")
                print(f"âŒ Failed requests: {failed_requests}/{len(results)}")
                print(f"â±ï¸  Duration: {duration:.2f}s")
                
                # Chunk é•¿åº¦ç»Ÿè®¡ï¼ˆç”¨äºéªŒè¯ chunk å¤§å°åˆ†å¸ƒï¼‰
                if chunk_lengths:
                    print(f"\nğŸ“ Chunk Length Statistics:")
                    print(f"   Avg: {avg_chunk_len:.2f} chars")
                    print(f"   Max: {max_chunk_len} chars")
                    print(f"   Distribution: 1 char={chunk_len_dist[1]}, 2 chars={chunk_len_dist[2]}, 3 chars={chunk_len_dist[3]}, >3 chars={chunk_len_dist['>3']}")
                
                print(f"\nâš¡ TTFC (Time To First Chunk) Statistics:")
                print(f"   Avg: {avg_ttft*1000:.1f}ms")
                print(f"   P50: {p50_ttft*1000:.1f}ms")
                print(f"   P95: {p95_ttft*1000:.1f}ms")
                print(f"   P99: {p99_ttft*1000:.1f}ms")
                
                # æŒ‰ç±»åˆ«ç»Ÿè®¡ TTFC
                print(f"\nâš¡ TTFC by Category:")
                for cat in ['very_good', 'good', 'bad', 'very_bad']:
                    cat_ttfts = [s.ttft for s in all_stats if s.category == cat and s.ttft > 0]
                    if cat_ttfts:
                        print(f"   {cat:10s}: Avg={np.mean(cat_ttfts)*1000:.1f}ms, P50={np.percentile(cat_ttfts, 50)*1000:.1f}ms, P95={np.percentile(cat_ttfts, 95)*1000:.1f}ms")
        else:
            print(f"ğŸ“Š Total chunks: {len(all_events)}")
            print(f"â±ï¸  Duration: {duration:.2f}s")
        
        return all_events, all_stats, duration
    
    async def run_experiment_poisson(
        self, 
        mode: str, 
        shuffled_profiles: List[UserProfile]
    ) -> Tuple[List[ChunkEvent], List[RequestStats], float]:
        """è¿è¡Œå®éªŒ (Poisson Arrival Mode)"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ Running {mode.upper()} - {self.num_users} Users")
        print(f"ğŸŒŠ Mode: Poisson Arrival Process (Target QPS: {self.target_qps})")
        print(f"{'='*60}")
        
        # 1. é¢„å…ˆè®¡ç®—æ³Šæ¾åˆ°è¾¾æ—¶é—´
        # æ³Šæ¾è¿‡ç¨‹çš„é—´éš”æ—¶é—´æœä»æŒ‡æ•°åˆ†å¸ƒ
        # scale = 1 / lambda (QPS)
        np.random.seed(12345) # å›ºå®šç§å­ï¼Œä¿è¯ä¸¤ç§æ¨¡å¼ä¸‹çš„åˆ°è¾¾æ—¶é—´å®Œå…¨ä¸€è‡´
        inter_arrival_times = np.random.exponential(1.0 / self.target_qps, len(shuffled_profiles))
        
        # è®¡ç®—æ¯ä¸ªè¯·æ±‚ç›¸å¯¹äºå®éªŒå¼€å§‹çš„ç»å¯¹å‘å°„æ—¶é—´
        scheduled_start_times = np.cumsum(inter_arrival_times)
        total_expected_duration = scheduled_start_times[-1]
        
        connector = aiohttp.TCPConnector(limit=self.client_concurrency)
        async with aiohttp.ClientSession(connector=connector) as session:
            if not self.model_name:
                self.model_name = await self.detect_model(session)
                print(f"ğŸ“¦ Model: {self.model_name}")
            
            print(f"   vLLM max_num_seqs: {self.concurrency}")
            print(f"   Est. Request Injection Duration: {total_expected_duration:.2f}s")
            
            experiment_start_time = time.perf_counter()
            
            # ä¾ç„¶ä¿ç•™ Semaphore ä½œä¸ºå®‰å…¨ç½‘ï¼Œé˜²æ­¢ç³»ç»Ÿæ–‡ä»¶å¥æŸ„è€—å°½
            # ä½†ä¸»è¦æµé‡æ§åˆ¶ç”± sleep å†³å®š
            semaphore = asyncio.Semaphore(self.client_concurrency)
            
            tasks = []
            
            # 2. å¾ªç¯å‘é€è¯·æ±‚
            for i, profile in enumerate(shuffled_profiles):
                # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
                now = time.perf_counter() - experiment_start_time
                wait_time = scheduled_start_times[i] - now
                
                if wait_time > 0:
                    await asyncio.sleep(wait_time)
                
                # å‘å°„è¯·æ±‚ (Fire and Forget)
                # ä½¿ç”¨ create_task å°†å…¶æ”¾å…¥åå°è¿è¡Œï¼Œä¸»å¾ªç¯ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                task = asyncio.create_task(
                    self.send_request(session, profile, experiment_start_time, semaphore, mode)
                )
                tasks.append(task)
                
                # ç®€å•çš„è¿›åº¦æ‰“å°
                if (i + 1) % 100 == 0:
                    sys.stdout.write(f"\rğŸ“¤ Sent {i + 1}/{len(shuffled_profiles)} requests...")
                    sys.stdout.flush()

            print(f"\nâœ… All {len(tasks)} requests sent. Waiting for completion...")
            
            # 3. ç­‰å¾…æ‰€æœ‰åå°ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks)
            
            duration = time.perf_counter() - experiment_start_time

        # --- ä»¥ä¸‹ç»Ÿè®¡é€»è¾‘ä¿æŒä¸å˜ ---
        
        # åˆå¹¶æ‰€æœ‰äº‹ä»¶å’Œç»Ÿè®¡
        all_events = []
        all_stats = []
        completed_requests = 0
        failed_requests = 0
        for events, stats in results:
            if stats and len(events) > 0:
                all_events.extend(events)
                all_stats.append(stats)
                completed_requests += 1
            else:
                failed_requests += 1
        
        # è®¡ç®— TTFC ç»Ÿè®¡
        if all_stats:
            ttfts = [s.ttft for s in all_stats if s.ttft > 0]
            if ttfts:
                avg_ttft = np.mean(ttfts)
                p50_ttft = np.percentile(ttfts, 50)
                p95_ttft = np.percentile(ttfts, 95)
                p99_ttft = np.percentile(ttfts, 99)
                
                chunk_lengths = [e.chunk_length for e in all_events]
                if chunk_lengths:
                    avg_chunk_len = np.mean(chunk_lengths)
                    max_chunk_len = max(chunk_lengths)
                
                print(f"ğŸ“Š Total chunks: {len(all_events)}")
                print(f"âœ… Completed requests: {completed_requests}/{len(results)}")
                print(f"âŒ Failed requests: {failed_requests}/{len(results)}")
                print(f"â±ï¸  Actual Duration: {duration:.2f}s")
                
                print(f"\nâš¡ TTFC (Time To First Chunk) Statistics:")
                print(f"   Avg: {avg_ttft*1000:.1f}ms")
                print(f"   P50: {p50_ttft*1000:.1f}ms")
                print(f"   P95: {p95_ttft*1000:.1f}ms")
                print(f"   P99: {p99_ttft*1000:.1f}ms")
                
                print(f"\nâš¡ TTFC by Category:")
                for cat in ['very_good', 'good', 'bad', 'very_bad']:
                    cat_ttfts = [s.ttft for s in all_stats if s.category == cat and s.ttft > 0]
                    if cat_ttfts:
                        print(f"   {cat:10s}: Avg={np.mean(cat_ttfts)*1000:.1f}ms, P50={np.percentile(cat_ttfts, 50)*1000:.1f}ms")
        else:
            print(f"ğŸ“Š Total chunks: {len(all_events)}")
            print(f"â±ï¸  Duration: {duration:.2f}s")
        
        return all_events, all_stats, duration
    
    def compute_cumulative_curve(
        self, 
        events: List[ChunkEvent], 
        time_points: np.ndarray,
        use_synthetic_arrival: bool = True
    ) -> np.ndarray:
        """è®¡ç®—ç´¯è®¡æœ‰æ•ˆ chunk æ›²çº¿
        
        Args:
            events: chunk äº‹ä»¶åˆ—è¡¨
            time_points: æ—¶é—´é‡‡æ ·ç‚¹
            use_synthetic_arrival: True ä½¿ç”¨åˆæˆåˆ°è¾¾æ—¶é—´ï¼ˆåŠ å…¥ RTTï¼‰ï¼ŒFalse ä½¿ç”¨è§‚æµ‹åˆ°è¾¾æ—¶é—´ï¼ˆâ‰ˆ GPU ç”Ÿæˆæ—¶é—´ï¼‰
        """
        cumulative = np.zeros(len(time_points))
        
        if use_synthetic_arrival:
            times = sorted([e.synthetic_arrival_time for e in events])
        else:
            times = sorted([e.observed_arrival_time for e in events])
        
        event_idx = 0
        for i, t in enumerate(time_points):
            while event_idx < len(times) and times[event_idx] <= t:
                event_idx += 1
            cumulative[i] = event_idx
        
        return cumulative
    
    async def run_comparison(self):
        """è¿è¡Œå¯¹æ¯”å®éªŒï¼šç”Ÿæˆä¸¤ä»½æŠ¥å‘Šï¼ˆå…¨éƒ¨ç”¨æˆ· vs æ ¸å¿ƒç”¨æˆ·ï¼‰å’Œä¸¤å¼ å›¾è¡¨"""
        print("\n" + "ğŸ•" * 30)
        print("     TIMELINE EXPERIMENT")
        print("     éªŒè¯ Network-Aware çš„çœŸæ­£ä¼˜åŠ¿ (Dual Report Mode)")
        print("ğŸ•" * 30)
        
        # 1. å‡†å¤‡å®éªŒç¯å¢ƒ
        np.random.seed(42)
        # ä½¿ç”¨ä½ ç¡®è®¤è¿‡çš„åŒå³°åˆ†å¸ƒç”Ÿæˆå‡½æ•°
        self.user_profiles = generate_user_profiles_multimodal(self.num_users)
        
        # æ‰“å°ç”¨æˆ·åˆ†å¸ƒ
        categories = {}
        for p in self.user_profiles:
            categories[p.category] = categories.get(p.category, 0) + 1
        print(f"\nğŸ“Š ç”¨æˆ·åˆ†å¸ƒ: {categories}")
        
        # å›ºå®šè¯·æ±‚é¡ºåº
        import random
        shuffled_profiles = self.user_profiles.copy()
        random.Random(12345).shuffle(shuffled_profiles)
        print(f"ğŸ”€ Request arrival order: Fixed seed (12345)")
        
        # 2. è¿è¡Œä¸¤è½®å®éªŒ
        baseline_events, baseline_stats, baseline_duration = await self.run_experiment_poisson("baseline", shuffled_profiles)
        await asyncio.sleep(2)
        network_events, network_stats, network_duration = await self.run_experiment_poisson("network_aware", shuffled_profiles)

        # =================================================================================
        # å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼šæ‰“å°ç»Ÿè®¡æŠ¥å‘Š (å·²ä¿®å¤ç¼ºå¤±çš„ä¸­å€¼/æœ€ç»ˆå€¼ç»Ÿè®¡)
        # =================================================================================
        def print_report(title, b_events, n_events, b_stats, n_stats, duration, category_filter=None):
            print("\n" + "=" * 60)
            print(f"ğŸ“Š REPORT: {title}")
            print("=" * 60)
            
            if not b_events or not n_events:
                print("No events to report.")
                return

            # --- [æ ¸å¿ƒä¿®å¤] é‡æ–°è®¡ç®—æ›²çº¿ä»¥è·å–æ—¶é—´åˆ‡ç‰‡æ•°æ® ---
            time_points = np.linspace(0, duration, 500)
            b_arr = self.compute_cumulative_curve(b_events, time_points, use_synthetic_arrival=True)
            n_arr = self.compute_cumulative_curve(n_events, time_points, use_synthetic_arrival=True)
            diff = n_arr - b_arr
            
            # 1. ä¸­é—´ç‚¹ç»Ÿè®¡ (t=50%)
            mid_idx = len(time_points) // 2
            mid_time = time_points[mid_idx]
            print(f"\nğŸ“ åœ¨ t={mid_time:.1f}s æ—¶ (Mid-point):")
            print(f"   Baseline Arrive: {int(b_arr[mid_idx])} chunks")
            print(f"   Network-Aware Arrive: {int(n_arr[mid_idx])} chunks")
            pct_diff = diff[mid_idx]/max(b_arr[mid_idx], 1)*100
            print(f"   å·®å€¼: {int(diff[mid_idx])} chunks ({pct_diff:+.1f}%)")

            # 2. æœ€ç»ˆç‚¹ç»Ÿè®¡ (t=100%)
            print(f"\nğŸ æœ€ç»ˆç»“æœ (t={duration:.1f}s):")
            print(f"   Baseline Arrive: {int(b_arr[-1])} chunks")
            print(f"   Network-Aware Arrive: {int(n_arr[-1])} chunks")
            print(f"   å·®å€¼: {int(diff[-1])} chunks")

            # 3. å¹³å‡é¢†å…ˆé‡
            avg_lead = np.mean(diff)
            lead_time_pct = np.mean(diff > 0) * 100
            print(f"\nğŸ“ˆ æ•´ä½“è¶‹åŠ¿:")
            print(f"   å¹³å‡é¢†å…ˆé‡: {avg_lead:.1f} chunks")
            print(f"   é¢†å…ˆæ—¶é—´æ¯”ä¾‹: {lead_time_pct:.1f}%")

            # 4. TTFT ç»Ÿè®¡
            b_ttfts = [s.ttft for s in b_stats if s.ttft > 0]
            n_ttfts = [s.ttft for s in n_stats if s.ttft > 0]
            
            if b_ttfts and n_ttfts:
                print(f"\nâš¡ TTFT (Time To First Token) Statistics:")
                print(f"   Baseline:      Avg={np.mean(b_ttfts)*1000:.1f}ms, P99={np.percentile(b_ttfts, 99)*1000:.1f}ms")
                print(f"   Network-Aware: Avg={np.mean(n_ttfts)*1000:.1f}ms, P99={np.percentile(n_ttfts, 99)*1000:.1f}ms")
                improv = (np.mean(b_ttfts) - np.mean(n_ttfts)) / np.mean(b_ttfts) * 100
                print(f"   >>> TTFT Improvement: {improv:+.1f}%")

            # 5. æŒ‰ç±»åˆ«ç»†åˆ† (å¦‚æœæ˜¯å…¨é‡æŠ¥å‘Š)
            if category_filter is None:
                print(f"\nğŸ“¦ æŒ‰ç”¨æˆ·ç±»åˆ«ç»Ÿè®¡ (Chunks & Avg Delay):")
                categories = ['very_bad', 'bad', 'good', 'very_good']
                for cat in categories:
                    # ç»Ÿè®¡ chunk æ•°é‡
                    b_count = len([e for e in b_events if e.category == cat])
                    n_count = len([e for e in n_events if e.category == cat])
                    
                    # ç»Ÿè®¡å¹³å‡å»¶è¿Ÿä»£ä»· (Synthetic - Observed)
                    b_delays = [e.synthetic_arrival_time - e.observed_arrival_time for e in b_events if e.category == cat]
                    n_delays = [e.synthetic_arrival_time - e.observed_arrival_time for e in n_events if e.category == cat]
                    
                    b_avg_d = np.mean(b_delays)*1000 if b_delays else 0
                    n_avg_d = np.mean(n_delays)*1000 if n_delays else 0
                    
                    print(f"   {cat:10s}: Baseline {b_count:6d} chunks ({b_avg_d:4.0f}ms), Network-Aware {n_count:6d} chunks ({n_avg_d:4.0f}ms)")

        # =================================================================================
        # å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼šç»˜åˆ¶å›¾è¡¨ (ä¿æŒä¸å˜)
        # =================================================================================
        def plot_chart(title_prefix, filename, b_events, n_events, plot_max_time):
            time_points = np.linspace(0, plot_max_time, 500)
            
            # GPU è§†è§’
            b_obs = self.compute_cumulative_curve(b_events, time_points, use_synthetic_arrival=False)
            n_obs = self.compute_cumulative_curve(n_events, time_points, use_synthetic_arrival=False)
            
            # å®¢æˆ·ç«¯è§†è§’
            b_arr = self.compute_cumulative_curve(b_events, time_points, use_synthetic_arrival=True)
            n_arr = self.compute_cumulative_curve(n_events, time_points, use_synthetic_arrival=True)
            
            plt.figure(figsize=(14, 10))
            
            # Subplot 1
            plt.subplot(2, 2, 1)
            plt.plot(time_points, b_obs, 'r-', label='Baseline', linewidth=2)
            plt.plot(time_points, n_obs, 'g-', label='Network-Aware', linewidth=2)
            plt.ylabel('Cumulative Chunks Observed')
            plt.title(f'GPU Perspective: {title_prefix}\n')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # Subplot 2
            plt.subplot(2, 2, 2)
            plt.plot(time_points, b_arr, 'r-', label='Baseline', linewidth=2)
            plt.plot(time_points, n_arr, 'g-', label='Network-Aware', linewidth=2)
            plt.ylabel('Cumulative Chunks Arrived')
            plt.title(f'Client Perspective: {title_prefix}\n')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # Subplot 3
            plt.subplot(2, 2, 3)
            diff = n_arr - b_arr
            plt.fill_between(time_points, 0, diff, where=(diff > 0), color='green', alpha=0.5, label='Network-Aware Leads')
            plt.fill_between(time_points, 0, diff, where=(diff < 0), color='red', alpha=0.5, label='Baseline Leads')
            plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
            plt.ylabel('Chunk Difference')
            plt.title('Performance Gap')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # Subplot 4
            plt.subplot(2, 2, 4)
            etps_b = np.zeros_like(time_points)
            etps_n = np.zeros_like(time_points)
            for i, t in enumerate(time_points):
                if t > 0.5:
                    etps_b[i] = b_arr[i] / t
                    etps_n[i] = n_arr[i] / t
            plt.plot(time_points, etps_b, 'r-', label='Baseline ECPS', linewidth=2)
            plt.plot(time_points, etps_n, 'g-', label='Network-Aware ECPS', linewidth=2)
            plt.ylabel('ECPS (Chunks/s)')
            plt.xlabel('Time (s)')
            plt.title('Effective Throughput')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            save_path = f'/home/argustest/eBPF-TokenFlow/{filename}'
            plt.savefig(save_path, dpi=150)
            print(f"ğŸ“ˆ Chart saved to: {save_path}")
            plt.close()

        # =================================================================================
        # 3. è¾“å‡ºç¬¬ä¸€ä»½ç»“æœï¼šå…¨éƒ¨ç”¨æˆ· (All Users)
        # =================================================================================
        print_report("ALL USERS (Full Dataset)", 
                     baseline_events, network_events, 
                     baseline_stats, network_stats, 
                     max(baseline_duration, network_duration))
        
        plot_chart("ALL USERS", "timeline_comparison_all.png", 
                   baseline_events, network_events, 
                   max(baseline_duration, network_duration))

        # =================================================================================
        # 4. è¾“å‡ºç¬¬äºŒä»½ç»“æœï¼šæ ¸å¿ƒç”¨æˆ· (Core Users / Good Users Only)
        # =================================================================================
        core_categories = ['very_good', 'good']
        
        # è¿‡æ»¤æ•°æ®
        b_events_core = [e for e in baseline_events if e.category in core_categories]
        n_events_core = [e for e in network_events if e.category in core_categories]
        b_stats_core = [s for s in baseline_stats if s.category in core_categories]
        n_stats_core = [s for s in network_stats if s.category in core_categories]
        
        # ç¡®å®šæ ¸å¿ƒç”¨æˆ·çš„æ—¶é—´è½´ç»ˆç‚¹
        if n_events_core:
            core_max_time = max(e.synthetic_arrival_time for e in n_events_core) * 1.1
        else:
            core_max_time = max(baseline_duration, network_duration)
        
        print_report(f"CORE USERS ONLY ({core_categories})", 
                     b_events_core, n_events_core, 
                     b_stats_core, n_stats_core, 
                     core_max_time, category_filter=core_categories)
        
        plot_chart("CORE USERS ONLY (Top ~90%)", "timeline_comparison_core.png", 
                   b_events_core, n_events_core, 
                   core_max_time)

async def main():
    parser = argparse.ArgumentParser(description="Timeline Experiment")
    parser.add_argument("--vllm-url", default="http://localhost:8000/v1")
    parser.add_argument("--num-users", type=int, default=8192)
    parser.add_argument("--max-tokens", type=int, default=50)
    parser.add_argument("--concurrency", type=int, default=256, 
                       help="vLLM max_num_seqs (scheduler limit)")
    parser.add_argument("--client-concurrency", type=int, default=2048,
                       help="Client-side concurrency limit (Semaphore)")
    parser.add_argument("--qps", type=float, default=50.0, help="Target requests per second")
    args = parser.parse_args()
    
    experiment = TimelineExperiment(
        vllm_url=args.vllm_url,
        num_users=args.num_users,
        max_tokens=args.max_tokens,
        concurrency=args.concurrency,
        client_concurrency=args.client_concurrency,
        target_qps=args.qps,
    )
    
    await experiment.run_comparison()


if __name__ == "__main__":
    asyncio.run(main())